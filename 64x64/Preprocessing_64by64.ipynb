{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_64by64.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxdc2Q235NyI",
        "outputId": "df1ae46c-3353-4116-fa39-b944d67bed10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing for unseen_nominal_logs plus unseen_logs "
      ],
      "metadata": {
        "id": "K8Z-IvmzoJAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/SNN_data"
      ],
      "metadata": {
        "id": "PKGOF85B415h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go to that directory where log files are saved "
      ],
      "metadata": {
        "id": "8wmMeu5w1dwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Column64_seq/"
      ],
      "metadata": {
        "id": "Iwm561PkPXLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "WYDAgvgVPLi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,re, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "3_tArxWR2NO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getInfo(file, ip_spks):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    file : Log files \n",
        "    ip_spks : Number of input neurons or pre-synaptic neurons\n",
        "\n",
        "    return :\n",
        "    df : Dataframe\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    with open(file) as f:\n",
        "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    # Making column names\n",
        "    cols = [\"Time(s)\", \"Column Number\"]\n",
        "    for i in range(ip_spks):\n",
        "      cols.append(\"spk_ip_\" + str(i))\n",
        "      cols.append(\"#Spikes_\" + str(i))\n",
        "      cols.append(\"ISI_\" + str(i))\n",
        "\n",
        "    cols += [\"#Spikes_out\", \"ISI_out\"]\n",
        "\n",
        "    time = [int(i.split(\":\")[1]) for i in re.findall(r\"Time : \\d+\", soup.text)]\n",
        "\n",
        "    df[\"Time(s)\"] = time\n",
        "\n",
        "    # Column number info\n",
        "    col_num = [int(j.split(\":\")[1]) for j in re.findall(\"Column number: \\d+\",soup.text)]\n",
        "\n",
        "    df[\"Column Number\"] = col_num \n",
        "\n",
        "    # NeuronID info for each spike generating neuron\n",
        "    n_id_info = re.findall(\"NeuronID: (.+)\", soup.text)\n",
        "\n",
        "    spikes_info = [list(map(int,re.findall(r'(\\d+)', i))) for i in n_id_info]\n",
        "    \n",
        "    n_id_info_v1 = []\n",
        "\n",
        "    for idx in range(0, len(spikes_info), ip_spks):\n",
        "      \n",
        "      var = list(chain.from_iterable((x[0],len(x)-1, x[1:]) for x in spikes_info[idx: idx + ip_spks]))\n",
        "      \n",
        "      n_id_info_v1.append(var)\n",
        "  \n",
        "\n",
        "    df[cols[2:-2]] = n_id_info_v1\n",
        "\n",
        "\n",
        "    # Output column info\n",
        "    out = re.findall(\"Output Neuron at column (.+)\", soup.text)\n",
        "\n",
        "    spikes_info_out = [list(map(int, re.findall(r'(\\d+)', i))) for i in out]\n",
        "    # print(spikes_info_out)\n",
        "    out_info = []\n",
        "\n",
        "    for i_out in spikes_info_out:\n",
        "      out_info.append([len(i_out)-1, i_out[1:]])\n",
        "\n",
        "    # Checking #instances\n",
        "    assert len(out_info) == len(time) == len(col_num) == len(n_id_info_v1)\n",
        "    \n",
        "    df[cols[-2:]] = out_info\n",
        "\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "rSEgnp7T2Nbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To compute average ISI (Inter-spiking interval)\n",
        "def getAvgIsi(x):\n",
        "\n",
        "  if len(x) == 1:\n",
        "    return x[0]%1000\n",
        "\n",
        "  else:\n",
        "    return sum([(x[i+1]%1000)- (x[i]%1000) for i in range(len(x)-1)])/(len(x)-1)"
      ],
      "metadata": {
        "id": "1mV4_qvJ2Ned"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SaveRawFile(dataframe, i, fol, dir_name):\n",
        "  \"\"\"\n",
        "  Args :\n",
        "  dataframe: Dataframe\n",
        "  i : Post-synaptic neuron ID\n",
        "  fol : File name of post-synaptic neuron ID\n",
        "  dir_name : Directory name where formatted logs will be saved\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  columns = dataframe.columns\n",
        "  f = pd.DataFrame()\n",
        "\n",
        "  idx = 0\n",
        "  for e in columns:\n",
        "    \n",
        "    if e.startswith(\"ISI_\"):\n",
        "\n",
        "      if e == \"ISI_out\":\n",
        "          f[e] =  dataframe[e]\n",
        "          f['Avg_ISI_out'] =  dataframe['ISI_out'].apply(lambda x: getAvgIsi(x))\n",
        "        \n",
        "      else:\n",
        "        f[e] =  dataframe[e]\n",
        "        f['Avg_ISI_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: getAvgIsi(x))\n",
        "        idx += 1\n",
        "\n",
        "    else:\n",
        "      f[e] = dataframe[e]\n",
        "\n",
        "  f.to_excel(dir_name + \"/\" + fol + \"/\" + \"Column\" + str(i) + \".xlsx\", index = False)\n",
        "  "
      ],
      "metadata": {
        "id": "B4MVxTeX2Njb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fol_name = os.listdir(\"Unseen_nominal_logs/\")\n",
        "assert len(fol_name) == 64\n",
        "print(fol_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiXb1EHY2NmS",
        "outputId": "e7a05ded-6e50-4082-f318-adb06d54e30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Column_43', 'Column_56', 'Column_17', 'Column_37', 'Column_16', 'Column_30', 'Column_29', 'Column_32', 'Column_19', 'Column_26', 'Column_34', 'Column_2', 'Column_33', 'Column_57', 'Column_38', 'Column_59', 'Column_1', 'Column_9', 'Column_27', 'Column_35', 'Column_22', 'Column_6', 'Column_47', 'Column_24', 'Column_40', 'Column_36', 'Column_41', 'Column_20', 'Column_48', 'Column_31', 'Column_10', 'Column_28', 'Column_42', 'Column_23', 'Column_25', 'Column_12', 'Column_11', 'Column_60', 'Column_7', 'Column_62', 'Column_44', 'Column_14', 'Column_45', 'Column_15', 'Column_52', 'Column_4', 'Column_61', 'Column_21', 'Column_3', 'Column_0', 'Column_8', 'Column_53', 'Column_46', 'Column_50', 'Column_51', 'Column_63', 'Column_5', 'Column_18', 'Column_58', 'Column_55', 'Column_39', 'Column_13', 'Column_54', 'Column_49']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make a directory to save all unseen nominal log files for each pre-synaptic neuron or input neuron\n",
        "os.mkdir(\"files_64_nominal_raw\")"
      ],
      "metadata": {
        "id": "4nnrdV0eofGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving log files in the desirable and readable format"
      ],
      "metadata": {
        "id": "f4RXs9QW4Tof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for var in [(\"files_64_nominal_raw\", \"Unseen_nominal_logs/\")]:\n",
        "\n",
        "  dir_name, unseen_log = var\n",
        "  for fol in fol_name:\n",
        "      logs_list = [i for i in os.listdir(unseen_log + fol) if i.endswith(\".txt\")]\n",
        "\n",
        "      os.mkdir(dir_name + \"/\"+ fol) \n",
        "\n",
        "      # Saving raw data in excel file\n",
        "      for file_name in logs_list:\n",
        "        print(f\"Folder Name : {fol} | Column name : {file_name}.\")\n",
        "        col_num = int(file_name.split(\".txt\")[0][10:])\n",
        "        df = getInfo(unseen_log + fol + \"/\" + file_name, 64)\n",
        "        SaveRawFile(df, col_num, fol, dir_name)"
      ],
      "metadata": {
        "id": "-uG5abex2Nov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a20c817-49b8-4eb2-e853-6aece0438168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder Name : Column_43 | Column name : log_weight43.txt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).ndim\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[col] = igetitem(value, i)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder Name : Column_56 | Column name : log_weight56.txt.\n",
            "Folder Name : Column_17 | Column name : log_weight17.txt.\n",
            "Folder Name : Column_37 | Column name : log_weight37.txt.\n",
            "Folder Name : Column_16 | Column name : log_weight16.txt.\n",
            "Folder Name : Column_30 | Column name : log_weight30.txt.\n",
            "Folder Name : Column_29 | Column name : log_weight29.txt.\n",
            "Folder Name : Column_32 | Column name : log_weight32.txt.\n",
            "Folder Name : Column_19 | Column name : log_weight19.txt.\n",
            "Folder Name : Column_26 | Column name : log_weight26.txt.\n",
            "Folder Name : Column_34 | Column name : log_weight34.txt.\n",
            "Folder Name : Column_2 | Column name : log_weight2.txt.\n",
            "Folder Name : Column_33 | Column name : log_weight33.txt.\n",
            "Folder Name : Column_57 | Column name : log_weight57.txt.\n",
            "Folder Name : Column_38 | Column name : log_weight38.txt.\n",
            "Folder Name : Column_59 | Column name : log_weight59.txt.\n",
            "Folder Name : Column_1 | Column name : log_weight1.txt.\n",
            "Folder Name : Column_9 | Column name : log_weight9.txt.\n",
            "Folder Name : Column_27 | Column name : log_weight27.txt.\n",
            "Folder Name : Column_35 | Column name : log_weight35.txt.\n",
            "Folder Name : Column_22 | Column name : log_weight22.txt.\n",
            "Folder Name : Column_6 | Column name : log_weight6.txt.\n",
            "Folder Name : Column_47 | Column name : log_weight47.txt.\n",
            "Folder Name : Column_24 | Column name : log_weight24.txt.\n",
            "Folder Name : Column_40 | Column name : log_weight40.txt.\n",
            "Folder Name : Column_36 | Column name : log_weight36.txt.\n",
            "Folder Name : Column_41 | Column name : log_weight41.txt.\n",
            "Folder Name : Column_20 | Column name : log_weight20.txt.\n",
            "Folder Name : Column_48 | Column name : log_weight48.txt.\n",
            "Folder Name : Column_31 | Column name : log_weight31.txt.\n",
            "Folder Name : Column_10 | Column name : log_weight10.txt.\n",
            "Folder Name : Column_28 | Column name : log_weight28.txt.\n",
            "Folder Name : Column_42 | Column name : log_weight42.txt.\n",
            "Folder Name : Column_23 | Column name : log_weight23.txt.\n",
            "Folder Name : Column_25 | Column name : log_weight25.txt.\n",
            "Folder Name : Column_12 | Column name : log_weight12.txt.\n",
            "Folder Name : Column_11 | Column name : log_weight11.txt.\n",
            "Folder Name : Column_60 | Column name : log_weight60.txt.\n",
            "Folder Name : Column_7 | Column name : log_weight7.txt.\n",
            "Folder Name : Column_62 | Column name : log_weight62.txt.\n",
            "Folder Name : Column_44 | Column name : log_weight44.txt.\n",
            "Folder Name : Column_14 | Column name : log_weight14.txt.\n",
            "Folder Name : Column_45 | Column name : log_weight45.txt.\n",
            "Folder Name : Column_15 | Column name : log_weight15.txt.\n",
            "Folder Name : Column_52 | Column name : log_weight52.txt.\n",
            "Folder Name : Column_4 | Column name : log_weight4.txt.\n",
            "Folder Name : Column_61 | Column name : log_weight61.txt.\n",
            "Folder Name : Column_21 | Column name : log_weight21.txt.\n",
            "Folder Name : Column_3 | Column name : log_weight3.txt.\n",
            "Folder Name : Column_0 | Column name : log_weight0.txt.\n",
            "Folder Name : Column_8 | Column name : log_weight8.txt.\n",
            "Folder Name : Column_53 | Column name : log_weight53.txt.\n",
            "Folder Name : Column_46 | Column name : log_weight46.txt.\n",
            "Folder Name : Column_50 | Column name : log_weight50.txt.\n",
            "Folder Name : Column_51 | Column name : log_weight51.txt.\n",
            "Folder Name : Column_63 | Column name : log_weight63.txt.\n",
            "Folder Name : Column_5 | Column name : log_weight5.txt.\n",
            "Folder Name : Column_18 | Column name : log_weight18.txt.\n",
            "Folder Name : Column_58 | Column name : log_weight58.txt.\n",
            "Folder Name : Column_55 | Column name : log_weight55.txt.\n",
            "Folder Name : Column_39 | Column name : log_weight39.txt.\n",
            "Folder Name : Column_13 | Column name : log_weight13.txt.\n",
            "Folder Name : Column_54 | Column name : log_weight54.txt.\n",
            "Folder Name : Column_49 | Column name : log_weight49.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the name of file of post-synaptic neuron ID\n",
        "\n",
        "def atof(text):\n",
        "    try:\n",
        "        retval = float(text)\n",
        "    except ValueError:\n",
        "        retval = text\n",
        "    return retval\n",
        "\n",
        "def natural_keys(text):\n",
        "\n",
        "    return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]"
      ],
      "metadata": {
        "id": "DwHPqUbL2NrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To create a directory to save all features (Spike rate, Avg. ISI) and their ground truth (Observed ISI) \n",
        "os.mkdir(\"StackedFiles_nominal\")"
      ],
      "metadata": {
        "id": "M7sdcsgl2Nuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extracting features (Spike rate, Avg. ISI) and their ground truth (Observed ISI) and then\n",
        "apply breakout distribution and then data augmentation.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def StackedFile(path, nG, fol, dirName, numCp):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    path : File name path for post-synaptic neuron ID\n",
        "    nG : Number of post-synaptic neurons\n",
        "    fol : post-synaptic neuron ID\n",
        "    dirName : Directory name where stacked files will be saved\n",
        "    numCp : Number of mapped crosspoints\n",
        "\n",
        "    \"\"\"\n",
        "    files = [i for i in os.listdir(path) if i.endswith(\".xlsx\")]\n",
        "    files.sort(key=natural_keys)  \n",
        "\n",
        "    # print(files, files[0].split('.xlsx')[0][6:])\n",
        "\n",
        "    # increment = int(files[0].split('.xlsx')[0][6:]) + 1\n",
        "\n",
        "    for i,f in enumerate(files):\n",
        "\n",
        "        \"\"\"\n",
        "        Predicting only average ISI_out, not #Spikes_out\n",
        "        \"\"\"\n",
        "        \n",
        "        file = pd.read_excel(path + '/' + f)\n",
        "\n",
        "        cols_drop = []\n",
        "\n",
        "        for id in range(nG):\n",
        "          cols_drop.append(\"spk_ip_\" + str(id))\n",
        "          cols_drop.append(\"ISI_\" + str(id))\n",
        "\n",
        "        # cols_drop.append(\"ISI_out\")\n",
        "\n",
        "        file.drop(cols_drop, axis=1, inplace=True)\n",
        "\n",
        "        file_v1 = file.reset_index(drop=True)\n",
        "\n",
        "\n",
        "        X_train  = file_v1.drop(labels = ['Time(s)', 'Column Number', '#Spikes_out','Avg_ISI_out', \"ISI_out\"], axis=1).values\n",
        "        y_train =  file_v1['ISI_out'].values\n",
        "\n",
        "        X_train_v1 = np.concatenate([X_train[:, :numCp*2], np.zeros((X_train.shape[0], X_train.shape[1] - (numCp*2)))], axis =1)\n",
        "        \n",
        "        if i == 0:\n",
        "          X_tr, y_tr =  X_train_v1, y_train\n",
        " \n",
        "        else:\n",
        "          X_tr1, y_tr1 =  X_train_v1, y_train\n",
        "\n",
        "          X_tr = np.concatenate([X_tr, X_tr1])\n",
        "          y_tr = np.concatenate([y_tr, y_tr1])\n",
        "\n",
        "   \n",
        "    with open(dirName + \"/X_unseen_stacked_\" + fol + \".npy\", 'wb') as f:\n",
        "      np.save(f, X_tr)\n",
        "\n",
        "    with open(dirName+\"/y_unseen_stacked_\" + fol + \".npy\", 'wb') as f:\n",
        "      np.save(f, y_tr)\n"
      ],
      "metadata": {
        "id": "RRBTQEnz2Nwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var1 in [(\"files_64_nominal_raw\", \"StackedFiles_nominal\")]:\n",
        "\n",
        "  dr, dirName = var1\n",
        "\n",
        "  # Saving the features and their ground truth for each column of crossbar array\n",
        "  for fol in fol_name:\n",
        "    print(\"#\" * 25 + \" \" + fol + \" \" + \"#\"*25)\n",
        "    StackedFile(os.getcwd() + '/' + dr+\"/\" + fol + \"/\", 64, fol, dirName, numCp = 24)\n"
      ],
      "metadata": {
        "id": "LgfucFxO2Nz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae97a6ed-53ec-4275-88b5-40c594bba1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################### Column_43 #########################\n",
            "######################### Column_56 #########################\n",
            "######################### Column_17 #########################\n",
            "######################### Column_37 #########################\n",
            "######################### Column_16 #########################\n",
            "######################### Column_30 #########################\n",
            "######################### Column_29 #########################\n",
            "######################### Column_32 #########################\n",
            "######################### Column_19 #########################\n",
            "######################### Column_26 #########################\n",
            "######################### Column_34 #########################\n",
            "######################### Column_2 #########################\n",
            "######################### Column_33 #########################\n",
            "######################### Column_57 #########################\n",
            "######################### Column_38 #########################\n",
            "######################### Column_59 #########################\n",
            "######################### Column_1 #########################\n",
            "######################### Column_9 #########################\n",
            "######################### Column_27 #########################\n",
            "######################### Column_35 #########################\n",
            "######################### Column_22 #########################\n",
            "######################### Column_6 #########################\n",
            "######################### Column_47 #########################\n",
            "######################### Column_24 #########################\n",
            "######################### Column_40 #########################\n",
            "######################### Column_36 #########################\n",
            "######################### Column_41 #########################\n",
            "######################### Column_20 #########################\n",
            "######################### Column_48 #########################\n",
            "######################### Column_31 #########################\n",
            "######################### Column_10 #########################\n",
            "######################### Column_28 #########################\n",
            "######################### Column_42 #########################\n",
            "######################### Column_23 #########################\n",
            "######################### Column_25 #########################\n",
            "######################### Column_12 #########################\n",
            "######################### Column_11 #########################\n",
            "######################### Column_60 #########################\n",
            "######################### Column_7 #########################\n",
            "######################### Column_62 #########################\n",
            "######################### Column_44 #########################\n",
            "######################### Column_14 #########################\n",
            "######################### Column_45 #########################\n",
            "######################### Column_15 #########################\n",
            "######################### Column_52 #########################\n",
            "######################### Column_4 #########################\n",
            "######################### Column_61 #########################\n",
            "######################### Column_21 #########################\n",
            "######################### Column_3 #########################\n",
            "######################### Column_0 #########################\n",
            "######################### Column_8 #########################\n",
            "######################### Column_53 #########################\n",
            "######################### Column_46 #########################\n",
            "######################### Column_50 #########################\n",
            "######################### Column_51 #########################\n",
            "######################### Column_63 #########################\n",
            "######################### Column_5 #########################\n",
            "######################### Column_18 #########################\n",
            "######################### Column_58 #########################\n",
            "######################### Column_55 #########################\n",
            "######################### Column_39 #########################\n",
            "######################### Column_13 #########################\n",
            "######################### Column_54 #########################\n",
            "######################### Column_49 #########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UuFlyghtrFuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}