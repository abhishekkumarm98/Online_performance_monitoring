{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_tArxWR2NO4"
      },
      "outputs": [],
      "source": [
        "import os,re, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from bs4 import BeautifulSoup\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, roc_curve, auc\n",
        "matplotlib.rc('xtick', labelsize=16) \n",
        "matplotlib.rc('ytick', labelsize=16) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7mz3yUB2NUE"
      },
      "outputs": [],
      "source": [
        "# Go to that directory where log files (single stuck at fault manifest as Missing/Delayed and Spurious) are saved\n",
        "cd 64x64/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEgnp7T2Nbw"
      },
      "outputs": [],
      "source": [
        "def getInfo(file, ip_spks):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    file : Log files \n",
        "    ip_spks : Number of input neurons or pre-synaptic neurons\n",
        "\n",
        "    return :\n",
        "    df : Dataframe\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    with open(file) as f:\n",
        "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    # Making column names\n",
        "    cols = [\"Time(s)\"]\n",
        "    for i in range(ip_spks):\n",
        "      cols.append(\"spk_ip_\" + str(i))\n",
        "      cols.append(\"#Spikes_\" + str(i))\n",
        "      cols.append(\"ISI_\" + str(i))\n",
        "\n",
        "    for i in range(ip_spks):\n",
        "      cols += [\"spk_out_\" + str(i), \"#Spikes_out_\"+str(i), \"ISI_out_\"+str(i)]\n",
        "\n",
        "    time = [int(i.split(\":\")[1]) for i in re.findall(r\"Time : \\d+\", soup.text)]\n",
        "\n",
        "    df[\"Time(s)\"] = time\n",
        "\n",
        "    # NeuronID info for each spike generating neuron\n",
        "    n_id_info = re.findall(\"NeuronID: (.+)\", soup.text)\n",
        "\n",
        "    spikes_info = [list(map(int,re.findall(r'(\\d+)', i))) for i in n_id_info]\n",
        "    \n",
        "    n_id_info_v1 = []\n",
        "\n",
        "    for idx in range(0, len(spikes_info), ip_spks):\n",
        "      var = list(chain.from_iterable((x[0],len(x)-1, x[1:]) for x in spikes_info[idx: idx + ip_spks]))\n",
        "      n_id_info_v1.append(var)\n",
        "  \n",
        "    df[cols[1:-3 *ip_spks]] = n_id_info_v1\n",
        "\n",
        "    # Output column info\n",
        "    out = re.findall(\"Output Neuron at column (.+)\", soup.text)\n",
        "\n",
        "    spikes_info_out = [list(map(int, re.findall(r'(\\d+)', i))) for i in out]\n",
        "    out_info = []\n",
        "\n",
        "    for idx in range(0, len(spikes_info_out), ip_spks):\n",
        "      var1 = list(chain.from_iterable((x[0],len(x)-1, x[1:]) for x in spikes_info_out[idx: idx + ip_spks]))\n",
        "      out_info.append(var1)\n",
        "\n",
        "    # Checking #instances\n",
        "    assert len(out_info) == len(time) == len(n_id_info_v1)\n",
        "\n",
        "    df[cols[-3*ip_spks:]] = out_info\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mV4_qvJ2Ned"
      },
      "outputs": [],
      "source": [
        "# To compute average ISI (Inter-spiking interval)\n",
        "def getAvgIsi(x):\n",
        "\n",
        "  if len(x) == 0:\n",
        "    return 0\n",
        "\n",
        "  elif len(x) == 1:\n",
        "    return x[0]%1000\n",
        "\n",
        "  else:\n",
        "    x = [i%1000 for i in x]\n",
        "    if x[0] == 0:\n",
        "      x[0] = 1\n",
        "    return sum([(x[i+1])- (x[i]) for i in range(len(x)-1)])/(len(x)-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hsMpZnIkuZ3"
      },
      "outputs": [],
      "source": [
        "# To compute COV (coefficient of variation) or Variance\n",
        "def cov_or_variance(x, type_metric = \"cov\"):\n",
        "\n",
        "  if len(x) == 0:\n",
        "    return 0\n",
        "\n",
        "  elif len(x) == 1:\n",
        "    return x[0]%1000\n",
        "\n",
        "  else:\n",
        "    x = [i%1000 for i in x]\n",
        "    if x[0] == 0:\n",
        "      x[0] = 1\n",
        "    imd = np.array([x[i]%1000 for i in range(len(x))])\n",
        "    mu = np.mean(imd)\n",
        "    \n",
        "    if type_metric == \"cov\": \n",
        "      # coefficient of variation\n",
        "      cov = np.sqrt(np.sum(np.square(imd - mu))/ (len(imd) - 1))/mu\n",
        "      return cov\n",
        "    else:\n",
        "      # Variance of spikes\n",
        "      var_spk = np.sum(np.square(imd - mu))/ (len(imd) - 1)\n",
        "      return var_spk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_xl-KmO2Ngw"
      },
      "outputs": [],
      "source": [
        "# # To make a directory to save all log files (Missing/Delayed and Spurious) with different utilizations for each pre-synaptic neuron or input neuron\n",
        "utilization = range(10,51,10) \n",
        "\n",
        "for ut in utilization:\n",
        "  os.mkdir(\"64x64/\"+ str(ut) + \"_percent/files_64_raw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4MVxTeX2Njb"
      },
      "outputs": [],
      "source": [
        "def SaveRawFile(dataframe, fol, savePath, cp):\n",
        "  \"\"\"\n",
        "  Args :\n",
        "  dataframe: Dataframe\n",
        "  fol : File name of post-synaptic neuron ID\n",
        "  savePath : The path to save .xlsx file \n",
        "  cp :  Crosspoint\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  columns = dataframe.columns\n",
        "  f = pd.DataFrame()\n",
        "\n",
        "  idx = 0\n",
        "  for e in columns:\n",
        "    \n",
        "    if e.startswith(\"ISI_\"):\n",
        "\n",
        "      if e.startswith(\"ISI_out_\"):\n",
        "          f[e] =  dataframe[e]\n",
        "          f['Avg_ISI_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: getAvgIsi(x))\n",
        "          f['Cov_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
        "          f['Var_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n",
        "        \n",
        "      else:\n",
        "        f[e] =  dataframe[e]\n",
        "        f['Avg_ISI_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: getAvgIsi(x))\n",
        "        f['Cov_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
        "        f['Var_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n",
        "        idx += 1\n",
        "\n",
        "    else:\n",
        "      f[e] = dataframe[e]\n",
        "\n",
        "  f.to_excel(savePath + fol + \"_\" + str(cp) + \".xlsx\", index = False)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RXs9QW4Tof"
      },
      "source": [
        "# Saving log files in the desirable and readable format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C4aQW37UKUg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2df7e63-6565-4025-f846-f4f51a918468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% utilization (7 crosspoints) for Output_Missing.\n",
            "10% utilization (7 crosspoints) for Output_Spurious.\n",
            "20% utilization (13 crosspoints) for Output_Missing.\n",
            "20% utilization (13 crosspoints) for Output_Spurious.\n",
            "30% utilization (20 crosspoints) for Output_Missing.\n",
            "30% utilization (20 crosspoints) for Output_Spurious.\n",
            "40% utilization (26 crosspoints) for Output_Missing.\n",
            "40% utilization (26 crosspoints) for Output_Spurious.\n",
            "50% utilization (32 crosspoints) for Output_Missing.\n",
            "50% utilization (32 crosspoints) for Output_Spurious.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "cp : Crosspoints\n",
        "util : Utilization (10% or 20% or 30% or 40% or 50% of the crossbar array (in terms of the crosspoints))\n",
        "log_path : The path from where the log file will be read.\n",
        "save_path : The path where the log file will be saved.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cp_util = [(7,10), (13,20), (20,30), (26,40), (32,50)]\n",
        "type_fol = [\"Output_Missing\", \"Output_Spurious\"] \n",
        "\n",
        "for params in cp_util:\n",
        "  cp,util = params\n",
        "\n",
        "  for fol in type_fol:\n",
        "    logs_path = \"64x64/\" + str(util) + \"_percent/\" + fol + \"/log_weight.txt\"\n",
        "    savePath = \"64x64/\" + str(util) + \"_percent/\" + \"files_64_raw/\"\n",
        "\n",
        "    ## Saving raw data in excel file\n",
        "    print(f\"{util}% utilization ({cp} crosspoints) for {fol}.\")\n",
        "    df = getInfo(logs_path, 64)\n",
        "    SaveRawFile(df, fol, savePath, cp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwHPqUbL2NrW"
      },
      "outputs": [],
      "source": [
        "# Sort the name of file of post-synaptic neuron ID\n",
        "\n",
        "def atof(text):\n",
        "    try:\n",
        "        retval = float(text)\n",
        "    except ValueError:\n",
        "        retval = text\n",
        "    return retval\n",
        "\n",
        "def natural_keys(text):\n",
        "\n",
        "    return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # To create a directory to save all features (Spike rate, Avg. ISI/COV/Variance) and their ground truth (Observed ISI) \n",
        "utilization = range(10,51,10) \n",
        "crossbarName = \"64x64\"\n",
        "\n",
        "for ut in utilization:\n",
        "  os.mkdir(crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_avg_ISI\")\n",
        "  # os.mkdir(crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_cov_spk\")\n",
        "  # os.mkdir(crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_var_spk\")"
      ],
      "metadata": {
        "id": "Is1KEUnK_Gk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_fol = [\"Output_Missing\", \"Output_Spurious\"]\n",
        "for ut in utilization:\n",
        "  for fol in type_fol:\n",
        "    os.mkdir(crossbarName + \"/\" + str(ut) + \"_percent/StackedFiles_avg_ISI/\"+fol)\n",
        "    # os.mkdir(crossbarName + \"/\" + str(ut) + \"_percent/StackedFiles_cov_spk/\"+fol)\n",
        "    # os.mkdir(crossbarName + \"/\" + str(ut) + \"_percent/StackedFiles_var_spk/\"+fol)"
      ],
      "metadata": {
        "id": "Kt4ytS90_SNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRBTQEnz2Nwt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extracting features (Spike rate, Avg. ISI/COV/Variance) and their ground truth (Observed ISI) and then\n",
        "save in .npy format.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def StackedFile(path, fileName, nG, savePath, variance_feat = False):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "    path : The path to read excel files\n",
        "    nG : Number of post-synaptic neurons\n",
        "    variance_feat : Avg. ISI/COV/Variance as features included or not\n",
        "    fileName : An excel file (Output_Spurious, Output_Missing)\n",
        "    savePath : The path where the excel files will be saved in a numpy format in a parsed version based on cp.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Predicting only average ISI_out, not #Spikes_out\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Number of crosspoints (numCp)\n",
        "    numCp = int(fileName.split(\"_\")[-1][:-5]) \n",
        "    print(f\"File Name : {fileName} | Number of crosspoints : {numCp}\")\n",
        "\n",
        "    file = pd.read_excel(path + fileName)\n",
        "\n",
        "    cols_drop = []\n",
        "\n",
        "    for id in range(nG):\n",
        "      cols_drop.append(\"spk_ip_\" + str(id))\n",
        "      cols_drop.append(\"spk_out_\" + str(id))\n",
        "      cols_drop.append(\"ISI_\" + str(id))\n",
        "      cols_drop.append(\"ISI_out_\" + str(id))\n",
        "      cols_drop.append(\"#Spikes_out_\" + str(id))\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        cols_drop.append(\"Cov_spk_ip_\" + str(id))\n",
        "        cols_drop.append(\"Cov_spk_out_\" + str(id))\n",
        "        cols_drop.append(\"Avg_ISI_\" + str(id))\n",
        "        cols_drop.append(\"Avg_ISI_out_\" + str(id))\n",
        "\n",
        "      elif not variance_feat: \n",
        "        cols_drop.append(\"Cov_spk_ip_\" + str(id))\n",
        "        cols_drop.append(\"Cov_spk_out_\" + str(id))\n",
        "        cols_drop.append(\"Var_spk_ip_\" + str(id))\n",
        "        cols_drop.append(\"Var_spk_out_\" + str(id))\n",
        "      else:\n",
        "        cols_drop.append(\"Avg_ISI_\" + str(id))\n",
        "        cols_drop.append(\"Avg_ISI_out_\" + str(id))\n",
        "        cols_drop.append(\"Var_spk_ip_\" + str(id))\n",
        "        cols_drop.append(\"Var_spk_out_\" + str(id))\n",
        "        \n",
        "\n",
        "    cols_drop.extend(['Time(s)'])\n",
        "\n",
        "    file.drop(cols_drop, axis=1, inplace=True)\n",
        "\n",
        "    file_v1 = file.reset_index(drop=True)\n",
        "\n",
        "    if variance_feat == \"var\":\n",
        "        y_cols = [\"Var_spk_out_\" + str(idx) for idx in range(nG)]\n",
        "    elif not variance_feat: \n",
        "        y_cols = [\"Avg_ISI_out_\" + str(idx) for idx in range(nG)]\n",
        "    else:\n",
        "        y_cols = [\"Cov_spk_out_\" + str(idx) for idx in range(nG)]\n",
        "        \n",
        "\n",
        "    X_train  = file_v1.drop(y_cols, axis=1).values\n",
        "    y_train =  file_v1[y_cols].values\n",
        "    \n",
        "    X_train_v1 = np.concatenate([X_train[:, :numCp*2], np.zeros((X_train.shape[0], X_train.shape[1] - (numCp*2)))], axis =1)\n",
        "\n",
        "    \n",
        "    X_tr, y_tr =  X_train_v1, y_train\n",
        "\n",
        "      \n",
        "    with open(savePath + \"/X_train_stacked.npy\", 'wb') as f:\n",
        "      np.save(f, X_tr)\n",
        "\n",
        "    for col in range(nG):\n",
        "      with open(savePath +\"/y_train_stacked_\" + str(col) + \".npy\", 'wb') as f:\n",
        "        np.save(f, y_tr[:,col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgfucFxO2Nz4"
      },
      "outputs": [],
      "source": [
        "def run(utilization, variance_feat, nG, crossbarName):\n",
        "\n",
        "  for ut in utilization:\n",
        "    type_files = [i for i in os.listdir(crossbarName + \"/\"+ str(ut) + \"_percent/files_\"+str(nG)+\"_raw/\") if i.endswith(\"xlsx\")]\n",
        "    for file in type_files:\n",
        "      fol = \"_\".join(file.split(\"_\")[:-1])\n",
        "      if variance_feat == \"var\":\n",
        "        savePath = crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_var_spk/\"+fol\n",
        "      elif not variance_feat:\n",
        "        savePath = crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_avg_ISI/\"+fol\n",
        "      else:\n",
        "        savePath = crossbarName + \"/\"+ str(ut) + \"_percent/StackedFiles_var_spk/\"+fol\n",
        "\n",
        "      StackedFile(crossbarName + \"/\"+ str(ut) + \"_percent/files_\"+str(nG)+\"_raw/\", file, nG, savePath, variance_feat = variance_feat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering #Spikes & Avg ISI as features\n",
        "\n",
        "variance_feat = False # avg_ISI\n",
        "crossbarName = \"64x64\"\n",
        "utilization = range(10,51,10)\n",
        "nG = 64\n",
        "\n",
        "run(utilization, variance_feat, nG, crossbarName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbn-s2uSnXZU",
        "outputId": "42de7e8c-ee9d-49a2-e571-301f0226b9f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name : Output_Missing_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Spurious_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Missing_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Spurious_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Missing_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Spurious_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Missing_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Spurious_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Missing_32.xlsx | Number of crosspoints : 32\n",
            "File Name : Output_Spurious_32.xlsx | Number of crosspoints : 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering #Spikes & Variance as features\n",
        "\n",
        "# variance_feat = \"var\" # Variance\n",
        "# crossbarName = \"64x64\"\n",
        "# utilization = range(10,51,10)\n",
        "# nG = 64\n",
        "\n",
        "# run(utilization, variance_feat, nG, crossbarName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_MZRysYnLRn",
        "outputId": "16008b15-ec9d-4333-975f-b41ca0ff12f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name : Output_Missing_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Spurious_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Missing_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Spurious_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Missing_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Spurious_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Missing_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Spurious_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Missing_32.xlsx | Number of crosspoints : 32\n",
            "File Name : Output_Spurious_32.xlsx | Number of crosspoints : 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering #Spikes & COV as features\n",
        "\n",
        "# variance_feat = True # COV\n",
        "# crossbarName = \"64x64\"\n",
        "# utilization = range(10,51,10)\n",
        "# nG = 64\n",
        "# run(utilization, variance_feat, nG, crossbarName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SSm-AB9nMVI",
        "outputId": "e16b580d-c7ed-4510-f8c7-902f921344cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name : Output_Missing_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Spurious_7.xlsx | Number of crosspoints : 7\n",
            "File Name : Output_Missing_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Spurious_13.xlsx | Number of crosspoints : 13\n",
            "File Name : Output_Missing_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Spurious_20.xlsx | Number of crosspoints : 20\n",
            "File Name : Output_Missing_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Spurious_26.xlsx | Number of crosspoints : 26\n",
            "File Name : Output_Missing_32.xlsx | Number of crosspoints : 32\n",
            "File Name : Output_Spurious_32.xlsx | Number of crosspoints : 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9LeJwK2N2A"
      },
      "outputs": [],
      "source": [
        "def getBinary(y_true, y_pred, margin):\n",
        "  \n",
        "  \"\"\"\n",
        "  It will binarize the continuous values which are y_true and y_pred with\n",
        "  the help of margin and threshold to plot ROC (Receiver operating characteristic).\n",
        "\n",
        "  Args:\n",
        "  y_true : Observed avg. ISI (continuous values)\n",
        "  y_pred : Predicted avg. ISI (continuous values)\n",
        "  margin : The margin is the obtained optimal MAE or a user can take loose margin as per his choice to incur more false alaram or false fault\n",
        "\n",
        "  return:\n",
        "  y_t : Binarized y_true\n",
        "  y_p : Binarized y_pred\n",
        "\n",
        "  The reason to opt for ROC curve because it offers an elegant way to\n",
        "  plot true fault detection rate versus false fault detection rate.\n",
        " \n",
        "  \"\"\"\n",
        "\n",
        "  thresh = pd.Series(y_true).median()   # We have taken median as a threshold because it is not impacted by the outliers\n",
        "  y_t = []\n",
        "  y_p = []\n",
        "\n",
        "  for t,p in zip(y_true, y_pred):\n",
        "    if np.abs(t - p) <= margin:\n",
        "      if t> thresh:\n",
        "        y_t.append(1)\n",
        "        y_p.append(1)\n",
        "      else:\n",
        "        y_t.append(0)\n",
        "        y_p.append(0)\n",
        "\n",
        "    else:\n",
        "      if (t > thresh) and (p > thresh):  # ex: t=60, p=80 but thresh = 55 --> t,p -> 1, therefore one of them has to be opposite to another because abs(t-p)>15.\n",
        "        y_t.append(1)\n",
        "        y_p.append(0)\n",
        "\n",
        "      elif (t > thresh) and (p <= thresh):  # ex: t=60, p=40 but thresh = 55 --> t->1,  p-> 0, which satisfies the condition, one has to be opposite of another.\n",
        "        y_t.append(1)\n",
        "        y_p.append(0)\n",
        "\n",
        "      elif (t <= thresh) and (p > thresh):  # ex: t=30, p=57 but thresh = 55 --> t->0,  p-> 1,\n",
        "        y_t.append(0)\n",
        "        y_p.append(1)\n",
        "\n",
        "      elif (t <= thresh) and (p <= thresh): # ex: t=25, p=45 but thresh = 55 --> t->0, p-> 1, therefore one of them has to be opposite to another.\n",
        "        y_t.append(0)\n",
        "        y_p.append(1)\n",
        "      \n",
        "\n",
        "  y_t = np.array(y_t)\n",
        "  y_p = np.array(y_p)\n",
        "\n",
        "  return y_t, y_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BpLwfl02N49"
      },
      "outputs": [],
      "source": [
        "def plotROC(y_true, y_pred, reg, margin, color):\n",
        "\n",
        "  \"\"\"\n",
        "  To plot ROC\n",
        "\n",
        "  Args: \n",
        "  y_true : Observed avg. ISI\n",
        "  y_pred : Predicted avg. ISI\n",
        "  reg :  Regressor Name\n",
        "  margin : The margin is the obtained optimal MAE or a user can take loose margin as per his choice to incur more false alaram or false fault\n",
        "  color : Color\n",
        "\n",
        "  return:\n",
        "  y_t : Binarized y_true\n",
        "  y_p : Binarized y_pred\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  y_t, y_p = getBinary(y_true, y_pred, margin)\n",
        "  \n",
        "  fpr, tpr, _ = roc_curve(y_t, y_p )\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  print()\n",
        "  print(\"False positive rate : \", fpr[1])\n",
        "  print(\"True positive rate : \", tpr[1])\n",
        "  print(\"ROC Area under curve : \", roc_auc)\n",
        "\n",
        "  plt.figure(figsize = (6,6))\n",
        "  lw = 2\n",
        "  plt.plot(fpr, tpr, color= color, lw=lw, label=f\"ROC curve (area = %0.2f) for {reg}.\" % roc_auc,linewidth=4)\n",
        "  plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "  plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "  plt.title(\"Receiver operating characteristic example\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "  return y_t, y_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjhzaMqGDdf3"
      },
      "outputs": [],
      "source": [
        "def getPrediction(nG, ip_shape, scaling_mode, file_Type, crossbarName, model_type, model_name, optimal_maes, models_list, variance_feat =False):\n",
        "\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  nG : Number of post-synaptic neurons\n",
        "  scaling_mode : Type of scaling technique\n",
        "  file_type : \"Missing/Delayed\" file or \"Spurious\" file\n",
        "  crossbarName : Crossbar type 32x32 or 64x64 or 128x128\n",
        "  model_type : \"Tree\" based or \"NN\" based\n",
        "  model_name : which model to run\n",
        "  models_list : List of model names\n",
        "  optimal_maes : Obtained optimal MAEs for all columns\n",
        "  ip_shape : Dimension of an example (Mainly for CNN)\n",
        "  variance_feat : Coefficient of variation/Variance/Avg ISI as features included or not\n",
        "\n",
        "  print : \n",
        "  Fault detection rate\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  row, col, chn = ip_shape\n",
        "  utilization = range(10,51,10)\n",
        "\n",
        "  print(\"*\"*25 + \" \"+ file_Type.split(\"_\")[-1] +\" Spikes \" + \"*\"*25)\n",
        "\n",
        "  for util in utilization:\n",
        "    print(\"*\"*25 + \" Utilization : \" + str(util) + \"% \" + \"*\"*25)\n",
        "    print()\n",
        "    print(\"Scaling Mode : \", scaling_mode)\n",
        "    print(\"Model Name : \", model_name, \"\\n\")\n",
        "\n",
        "    if variance_feat == \"var\":\n",
        "      with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_var_spk/\" + file_Type + \"/X_train_stacked.npy\", 'rb') as f:\n",
        "        X = np.load(f)\n",
        "    elif not variance_feat:\n",
        "      with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_avg_ISI/\" + file_Type + \"/X_train_stacked.npy\", 'rb') as f:\n",
        "        X = np.load(f)\n",
        "    else:\n",
        "      with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_cov_spk/\" + file_Type + \"/X_train_stacked.npy\", 'rb') as f:\n",
        "        X = np.load(f)\n",
        "\n",
        "    prediction_wrt_column = []\n",
        "    \n",
        "    for id in range(nG):\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_var_spk/\" + file_Type +\"/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "          y = np.load(f)\n",
        "          y = y/10000  # (for better fit)\n",
        "      elif not variance_feat:\n",
        "        with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_avg_ISI/\" + file_Type +\"/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "          y = np.load(f)\n",
        "          y = y/10  # (for better fit)\n",
        "      else:\n",
        "        with open(crossbarName+ \"/\"+ str(util) + \"_percent/StackedFiles_cov_spk/\" + file_Type +\"/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "          y = np.load(f)\n",
        "\n",
        "      \n",
        "      X_test, y_test = X[:5000], y[:5000] \n",
        "\n",
        "      # Standard Scaling\n",
        "      if scaling_mode == 'std':\n",
        "        # print(\"Standard Scaling\",'\\n')\n",
        "        if variance_feat == \"var\":\n",
        "          with open(os.getcwd() + \"/Scaler_var_spk/\" + \"std_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "        elif not variance_feat:\n",
        "          with open(os.getcwd() + \"/Scaler_avg_ISI/\" + \"std_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "        else:\n",
        "          with open(os.getcwd() + \"/Scaler_cov_spk/\" + \"std_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "        \n",
        "        x_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "      # Min-max Scaling\n",
        "      elif scaling_mode == \"minmax\":\n",
        "        # print(\"Minmax Scaling \\n\")\n",
        "        if variance_feat == \"var\":\n",
        "          with open(os.getcwd() + \"/Scaler_var_spk/\" + \"minmax_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "        elif not variance_feat:\n",
        "          with open(os.getcwd() + \"/Scaler_avg_ISI/\" + \"minmax_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "        else:\n",
        "          with open(os.getcwd() + \"/Scaler_cov_spk/\" + \"minmax_Column_\" + str(0) + \".pkl\", 'rb') as file:  \n",
        "            scaler = pickle.load(file)\n",
        "\n",
        "        x_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        if model_type == \"Tree\":\n",
        "          with open(os.getcwd() + \"/Models_var_spk/\" + model_name + \"_column_\" + str(id) + \".pkl\", 'rb') as file:  \n",
        "            reg = pickle.load(file)\n",
        "        else:\n",
        "          if model_name == \"ANN\":\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_var_spk/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)\n",
        "          else:\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_var_spk/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)\n",
        "\n",
        "      elif not variance_feat:\n",
        "        if model_type == \"Tree\":\n",
        "          with open(os.getcwd() + \"/Models_avg_ISI/\" + model_name + \"_column_\" + str(id) + \".pkl\", 'rb') as file:  \n",
        "            reg = pickle.load(file)\n",
        "        else:\n",
        "          if model_name == \"ANN\":\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_avg_ISI/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)\n",
        "          else:\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_avg_ISI/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)        \n",
        "\n",
        "      else:\n",
        "        if model_type == \"Tree\":\n",
        "          with open(os.getcwd() + \"/Models_var_spk/\" + model_name + \"_column_\" + str(id) + \".pkl\", 'rb') as file:  \n",
        "            reg = pickle.load(file)\n",
        "        else:\n",
        "          if model_name == \"ANN\":\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_cov_spk/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)\n",
        "          else:\n",
        "            reg = tf.keras.models.load_model(os.getcwd() +  \"/Models_cov_spk/\" + model_name + \"_column_\" + str(id) +\".h5\", compile=False)\n",
        "\n",
        "\n",
        "    \n",
        "      if model_name == \"CNN\":\n",
        "        padding_te = np.zeros((x_test_scaled.shape[0], 8))\n",
        "        test_pred = reg.predict(np.concatenate([padding_te, x_test_scaled, padding_te], axis=1).reshape((x_test_scaled.shape[0],row, col, chn)), verbose=0).flatten()\n",
        "      elif model_name == \"ANN\":\n",
        "        test_pred = reg.predict(x_test_scaled, verbose=0).flatten()\n",
        "      else:\n",
        "        test_pred = reg.predict(x_test_scaled)\n",
        "\n",
        "      margin = optimal_maes[models_list.index(model_name)][id][-1]/2.5 # To tighten the margin\n",
        "        \n",
        "      y_t, y_p = getBinary(y_test, test_pred, margin=margin)\n",
        "\n",
        "      detectecd = (sum(y_t!= y_p))/(len(y_t))\n",
        "\n",
        "      prediction_wrt_column.append(detectecd)\n",
        "\n",
        "    print(prediction_wrt_column)\n",
        "    print(np.mean(prediction_wrt_column))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading MAEs_ISI to numpy array\n",
        "with open('MAEs_ISI.npy', 'rb') as f:\n",
        "    MAEs_ISI = np.load(f)\n",
        "\n",
        "MAEs_ISI.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDow5bl5jLPo",
        "outputId": "fd7324b0-0b2f-497b-9f37-d63275431d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained models list\n",
        "models_list = [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "nG = 64\n",
        "ip_shape = (12,12,1)\n",
        "scaling_mode = \"minmax\"\n",
        "fol_Type = \"Output_Missing\"\n",
        "crossbarName = \"64x64\"\n",
        "model_type = \"Tree\"\n",
        "model_name = models_list[1]\n",
        "\n",
        "\n",
        "getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_ISI, models_list, variance_feat =False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9IghAoHjLU4",
        "outputId": "2fe81a96-8db4-43dd-b915-34a9f4978c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Missing Spikes *************************\n",
            "************************* Utilization : 10% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.8962, 0.8956, 0.8944, 0.8946, 0.8926, 0.8936, 0.893, 0.895, 0.8956, 0.8962, 0.8902, 0.896, 0.8944, 0.8918, 0.892, 0.8906, 0.887, 0.892, 0.8916, 0.8908, 0.8884, 0.8916, 0.8872, 0.8902, 0.8888, 0.8886, 0.8924, 0.8892, 0.888, 0.8862, 0.885, 0.8862, 0.8862, 0.8878, 0.8898, 0.8916, 0.89, 0.8904, 0.8942, 0.8896, 0.8888, 0.894, 0.892, 0.891, 0.8908, 0.8908, 0.8906, 0.8934, 0.8866, 0.8848, 0.8926, 0.8946, 0.8882, 0.893, 0.8942, 0.8946, 0.8916, 0.8922, 0.8896, 0.8942, 0.8912, 0.8942, 0.8914, 0.8894]\n",
            "0.8912249999999999\n",
            "\n",
            "************************* Utilization : 20% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.8658, 0.8602, 0.8664, 0.8636, 0.8616, 0.8638, 0.8628, 0.8608, 0.867, 0.8674, 0.8702, 0.8742, 0.8694, 0.8692, 0.8642, 0.871, 0.8676, 0.8602, 0.87, 0.8612, 0.8638, 0.863, 0.8722, 0.8556, 0.8584, 0.8686, 0.8648, 0.8624, 0.8682, 0.864, 0.862, 0.8662, 0.8626, 0.8644, 0.8668, 0.8588, 0.8596, 0.8618, 0.8658, 0.8606, 0.8638, 0.8606, 0.8572, 0.86, 0.8602, 0.8602, 0.8634, 0.8592, 0.8608, 0.8576, 0.856, 0.861, 0.8632, 0.8632, 0.8634, 0.8562, 0.8598, 0.8614, 0.8652, 0.8638, 0.869, 0.8538, 0.8624, 0.8548]\n",
            "0.8631625\n",
            "\n",
            "************************* Utilization : 30% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.7946, 0.7894, 0.8066, 0.7914, 0.7936, 0.8052, 0.7852, 0.792, 0.787, 0.7914, 0.7934, 0.7902, 0.7964, 0.7858, 0.7948, 0.786, 0.788, 0.7926, 0.7912, 0.8028, 0.7884, 0.7968, 0.7984, 0.7834, 0.7922, 0.791, 0.7912, 0.794, 0.7832, 0.7914, 0.7896, 0.7776, 0.7892, 0.79, 0.784, 0.7916, 0.784, 0.7866, 0.7886, 0.7794, 0.7854, 0.7856, 0.7836, 0.7888, 0.7878, 0.7834, 0.792, 0.7838, 0.783, 0.7998, 0.7908, 0.7838, 0.786, 0.7856, 0.7904, 0.7942, 0.7858, 0.7884, 0.798, 0.7974, 0.7888, 0.7908, 0.7972, 0.7848]\n",
            "0.790053125\n",
            "\n",
            "************************* Utilization : 40% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.7332, 0.7402, 0.742, 0.7378, 0.7434, 0.739, 0.741, 0.743, 0.7464, 0.7476, 0.746, 0.7438, 0.7388, 0.7466, 0.743, 0.7426, 0.7392, 0.7388, 0.7308, 0.7406, 0.7364, 0.7364, 0.7434, 0.7434, 0.7386, 0.7442, 0.7402, 0.7382, 0.7404, 0.7444, 0.7394, 0.7376, 0.7434, 0.7442, 0.738, 0.7334, 0.7424, 0.742, 0.7454, 0.7392, 0.7454, 0.7424, 0.7564, 0.7392, 0.7394, 0.7442, 0.7492, 0.7478, 0.7414, 0.7374, 0.7414, 0.7424, 0.758, 0.754, 0.744, 0.7438, 0.7528, 0.7486, 0.7426, 0.7474, 0.7536, 0.7394, 0.7494, 0.7544]\n",
            "0.7429531249999999\n",
            "\n",
            "************************* Utilization : 50% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.6952, 0.6876, 0.6976, 0.7026, 0.6992, 0.686, 0.6866, 0.6922, 0.6984, 0.6982, 0.7008, 0.7046, 0.703, 0.6908, 0.696, 0.6884, 0.6842, 0.7, 0.694, 0.6972, 0.6888, 0.7002, 0.7038, 0.6882, 0.6986, 0.6968, 0.685, 0.696, 0.696, 0.6996, 0.6942, 0.7002, 0.6994, 0.6984, 0.7072, 0.694, 0.7032, 0.6976, 0.709, 0.697, 0.7086, 0.7078, 0.7056, 0.6958, 0.7066, 0.6904, 0.699, 0.7032, 0.6902, 0.7016, 0.6962, 0.6984, 0.6928, 0.7174, 0.7116, 0.7124, 0.723, 0.695, 0.7098, 0.7002, 0.6942, 0.6956, 0.699, 0.699]\n",
            "0.69858125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained models list\n",
        "models_list = [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "nG = 64\n",
        "ip_shape = (12,12,1)\n",
        "scaling_mode = \"minmax\"\n",
        "fol_Type = \"Output_Spurious\"\n",
        "crossbarName = \"64x64\"\n",
        "model_type = \"Tree\"\n",
        "model_name = models_list[1]\n",
        "\n",
        "\n",
        "getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_ISI, models_list, variance_feat =False)"
      ],
      "metadata": {
        "id": "cvS83hmsjLXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb23fa31-1211-4a45-fd64-04c0ac0e9085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Spurious Spikes *************************\n",
            "************************* Utilization : 10% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.893, 0.8968, 0.8972, 0.8972, 0.8922, 0.8968, 0.8928, 0.8884, 0.8904, 0.891, 0.89, 0.8892, 0.8908, 0.891, 0.8952, 0.892, 0.8924, 0.893, 0.8942, 0.8952, 0.8906, 0.893, 0.8914, 0.8942, 0.8918, 0.892, 0.8896, 0.8864, 0.8856, 0.885, 0.8872, 0.8886, 0.8922, 0.8872, 0.8882, 0.8914, 0.8908, 0.8942, 0.8956, 0.8926, 0.8908, 0.8898, 0.894, 0.89, 0.8908, 0.8916, 0.8894, 0.887, 0.8918, 0.8914, 0.8892, 0.8888, 0.8874, 0.8902, 0.8882, 0.89, 0.8942, 0.8894, 0.8896, 0.8898, 0.8882, 0.8924, 0.8944, 0.8916]\n",
            "0.89119375\n",
            "\n",
            "************************* Utilization : 20% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.8796, 0.8758, 0.874, 0.8816, 0.8822, 0.8752, 0.8704, 0.88, 0.8752, 0.8766, 0.8736, 0.8706, 0.8724, 0.873, 0.87, 0.8688, 0.8634, 0.8714, 0.8698, 0.87, 0.8692, 0.8672, 0.8786, 0.869, 0.867, 0.8704, 0.8676, 0.8678, 0.8688, 0.868, 0.8714, 0.8664, 0.8708, 0.8602, 0.87, 0.8652, 0.8672, 0.863, 0.8578, 0.8608, 0.8616, 0.8566, 0.8608, 0.8642, 0.8588, 0.8616, 0.86, 0.8626, 0.8596, 0.8606, 0.8612, 0.8594, 0.8528, 0.8552, 0.8588, 0.8506, 0.8474, 0.855, 0.8514, 0.8608, 0.8528, 0.8588, 0.8502, 0.8466]\n",
            "0.865428125\n",
            "\n",
            "************************* Utilization : 30% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.8248, 0.823, 0.816, 0.8218, 0.821, 0.8168, 0.8218, 0.8174, 0.8198, 0.821, 0.8162, 0.8214, 0.8226, 0.8218, 0.82, 0.8158, 0.8106, 0.8082, 0.8214, 0.8138, 0.8136, 0.8206, 0.8272, 0.8258, 0.8312, 0.8204, 0.815, 0.8218, 0.8186, 0.822, 0.8166, 0.8236, 0.8142, 0.8168, 0.819, 0.8166, 0.8152, 0.811, 0.8178, 0.8172, 0.8254, 0.8204, 0.82, 0.8162, 0.8184, 0.8168, 0.818, 0.8224, 0.8174, 0.8188, 0.8132, 0.8154, 0.8078, 0.81, 0.809, 0.8186, 0.8214, 0.821, 0.8158, 0.8072, 0.817, 0.821, 0.812, 0.8142]\n",
            "0.818075\n",
            "\n",
            "************************* Utilization : 40% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.7674, 0.774, 0.7692, 0.7712, 0.7656, 0.7648, 0.7672, 0.7678, 0.7644, 0.7612, 0.7558, 0.7632, 0.7572, 0.763, 0.7602, 0.7688, 0.7678, 0.7712, 0.7618, 0.7756, 0.7698, 0.763, 0.7596, 0.7656, 0.7606, 0.762, 0.7652, 0.7678, 0.7656, 0.769, 0.7662, 0.7654, 0.7662, 0.755, 0.7644, 0.7662, 0.7676, 0.7648, 0.7736, 0.7666, 0.7704, 0.7646, 0.7674, 0.7706, 0.7576, 0.7632, 0.7512, 0.767, 0.7582, 0.7496, 0.7584, 0.755, 0.7496, 0.752, 0.7546, 0.7508, 0.7496, 0.7514, 0.755, 0.7588, 0.7548, 0.7514, 0.76, 0.7468]\n",
            "0.7623375000000001\n",
            "\n",
            "************************* Utilization : 50% *************************\n",
            "\n",
            "Scaling Mode :  minmax\n",
            "Model Name :  LGBMRegressor \n",
            "\n",
            "[0.6878, 0.6974, 0.698, 0.6944, 0.6904, 0.689, 0.6954, 0.7034, 0.7022, 0.7032, 0.6972, 0.699, 0.704, 0.7038, 0.7084, 0.7024, 0.7046, 0.6946, 0.6916, 0.6946, 0.696, 0.686, 0.6884, 0.698, 0.6956, 0.6874, 0.6848, 0.6774, 0.6852, 0.6844, 0.6906, 0.6944, 0.6926, 0.6966, 0.6826, 0.6978, 0.6922, 0.688, 0.6992, 0.6878, 0.6974, 0.7008, 0.6944, 0.6994, 0.6986, 0.7046, 0.7054, 0.7006, 0.7118, 0.7084, 0.7098, 0.7112, 0.6968, 0.69, 0.6844, 0.6862, 0.6772, 0.691, 0.6982, 0.6958, 0.6926, 0.6794, 0.6856, 0.6922]\n",
            "0.694971875\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading MAEs to numpy array\n",
        "# with open('MAEs_COV.npy', 'rb') as f:\n",
        "#     MAEs_COV = np.load(f)\n",
        "# MAEs_COV.shape"
      ],
      "metadata": {
        "id": "Q7yXxp2Zoaql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models_list =  [\"XGBRegressor\", \"GradientBoostingRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "# nG = 64\n",
        "# ip_shape = (12,12,1)\n",
        "# scaling_mode = \"std\"\n",
        "# fol_Type = \"Output_Spurious\"\n",
        "# crossbarName = \"64x64\"\n",
        "# model_type = \"NN\"\n",
        "# model_name = models_list[3]\n",
        "\n",
        "\n",
        "# getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_COV, models_list, variance_feat =True)"
      ],
      "metadata": {
        "id": "AXhojOwjjLZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models_list =  [\"XGBRegressor\", \"GradientBoostingRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "# nG = 64\n",
        "# ip_shape = (12,12,1)\n",
        "# scaling_mode = \"std\"\n",
        "# fol_Type = \"Output_Missing\"\n",
        "# crossbarName = \"64x64\"\n",
        "# model_type = \"NN\"\n",
        "# model_name = models_list[3]\n",
        "\n",
        "\n",
        "# getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_COV, models_list, variance_feat =True)"
      ],
      "metadata": {
        "id": "HHBEML2gjLcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading MAEs_VAR to numpy array\n",
        "# with open('MAEs_VAR.npy', 'rb') as f:\n",
        "#     MAEs_VAR = np.load(f)\n",
        "# MAEs_VAR.shape"
      ],
      "metadata": {
        "id": "X0aZY4aIAyR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models_list =  [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "# nG = 64\n",
        "# ip_shape = (12,12,1)\n",
        "# scaling_mode = \"std\"\n",
        "# fol_Type = \"Output_Spurious\"\n",
        "# crossbarName = \"64x64\"\n",
        "# model_type = \"NN\"\n",
        "# model_name = models_list[4]\n",
        "\n",
        "\n",
        "# getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_VAR, models_list, variance_feat =\"var\")"
      ],
      "metadata": {
        "id": "khnGFBNxAyVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models_list =  [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "# nG = 64\n",
        "# ip_shape = (12,12,1)\n",
        "# scaling_mode = \"std\"\n",
        "# fol_Type = \"Output_Missing\"\n",
        "# crossbarName = \"64x64\"\n",
        "# model_type = \"NN\"\n",
        "# model_name = models_list[4]\n",
        "\n",
        "\n",
        "# getPrediction(nG, ip_shape, scaling_mode, fol_Type, crossbarName, model_type, model_name, MAEs_VAR, models_list, variance_feat =\"var\")"
      ],
      "metadata": {
        "id": "VKi9eSb2owq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEJIPvh_p2C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4azal96Qow0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}