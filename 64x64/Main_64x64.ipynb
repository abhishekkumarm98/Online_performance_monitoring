{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_tArxWR2NO4"
      },
      "outputs": [],
      "source": [
        "import os,re, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from bs4 import BeautifulSoup\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, roc_curve, auc\n",
        "matplotlib.rc('xtick', labelsize=16) \n",
        "matplotlib.rc('ytick', labelsize=16) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7mz3yUB2NUE"
      },
      "outputs": [],
      "source": [
        "# Go to that directory where log files are saved\n",
        "cd 64x64/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEgnp7T2Nbw"
      },
      "outputs": [],
      "source": [
        "def getInfo(file, ip_spks):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    file : Log files \n",
        "    ip_spks : Number of input neurons or pre-synaptic neurons\n",
        "\n",
        "    return :\n",
        "    df : Dataframe\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    with open(file) as f:\n",
        "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    # Making column names\n",
        "    cols = [\"Time(s)\"]\n",
        "    for i in range(ip_spks):\n",
        "      cols.append(\"spk_ip_\" + str(i))\n",
        "      cols.append(\"#Spikes_\" + str(i))\n",
        "      cols.append(\"ISI_\" + str(i))\n",
        "\n",
        "    for i in range(ip_spks):\n",
        "      cols += [\"spk_out_\" + str(i), \"#Spikes_out_\"+str(i), \"ISI_out_\"+str(i)]\n",
        "\n",
        "    time = [int(i.split(\":\")[1]) for i in re.findall(r\"Time : \\d+\", soup.text)]\n",
        "\n",
        "    df[\"Time(s)\"] = time\n",
        "\n",
        "    # NeuronID info for each spike generating neuron\n",
        "    n_id_info = re.findall(\"NeuronID: (.+)\", soup.text)\n",
        "\n",
        "    spikes_info = [list(map(int,re.findall(r'(\\d+)', i))) for i in n_id_info]\n",
        "    \n",
        "    n_id_info_v1 = []\n",
        "\n",
        "    for idx in range(0, len(spikes_info), ip_spks):\n",
        "      var = list(chain.from_iterable((x[0],len(x)-1, x[1:]) for x in spikes_info[idx: idx + ip_spks]))\n",
        "      n_id_info_v1.append(var)\n",
        "  \n",
        "    df[cols[1:-3 *ip_spks]] = n_id_info_v1\n",
        "\n",
        "    # Output column info\n",
        "    out = re.findall(\"Output Neuron at column (.+)\", soup.text)\n",
        "\n",
        "    spikes_info_out = [list(map(int, re.findall(r'(\\d+)', i))) for i in out]\n",
        "    out_info = []\n",
        "\n",
        "    for idx in range(0, len(spikes_info_out), ip_spks):\n",
        "      var1 = list(chain.from_iterable((x[0],len(x)-1, x[1:]) for x in spikes_info_out[idx: idx + ip_spks]))\n",
        "      out_info.append(var1)\n",
        "\n",
        "    # Checking #instances\n",
        "    assert len(out_info) == len(time) == len(n_id_info_v1)\n",
        "\n",
        "    df[cols[-3*ip_spks:]] = out_info\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mV4_qvJ2Ned"
      },
      "outputs": [],
      "source": [
        "# To compute average ISI (Inter-spiking interval)\n",
        "def getAvgIsi(x):\n",
        "\n",
        "  if len(x) == 0:\n",
        "    return 0\n",
        "\n",
        "  elif len(x) == 1:\n",
        "    return x[0]%1000\n",
        "\n",
        "  else:\n",
        "    x = [i%1000 for i in x]\n",
        "    if x[0] == 0:\n",
        "      x[0] = 1\n",
        "    return sum([(x[i+1])- (x[i]) for i in range(len(x)-1)])/(len(x)-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hsMpZnIkuZ3"
      },
      "outputs": [],
      "source": [
        "# To compute COV (coefficient of variation) or Variance\n",
        "def cov_or_variance(x, type_metric = \"cov\"):\n",
        "\n",
        "  if len(x) == 0:\n",
        "    return 0\n",
        "\n",
        "  elif len(x) == 1:\n",
        "    return x[0]%1000\n",
        "\n",
        "  else:\n",
        "    x = [i%1000 for i in x]\n",
        "    if x[0] == 0:\n",
        "      x[0] = 1\n",
        "    imd = np.array([x[i]%1000 for i in range(len(x))])\n",
        "    mu = np.mean(imd)\n",
        "    \n",
        "    if type_metric == \"cov\": \n",
        "      # coefficient of variation\n",
        "      cov = np.sqrt(np.sum(np.square(imd - mu))/ (len(imd) - 1))/mu\n",
        "      return cov\n",
        "    else:\n",
        "      # Variance of spikes\n",
        "      var_spk = np.sum(np.square(imd - mu))/ (len(imd) - 1)\n",
        "      return var_spk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_xl-KmO2Ngw"
      },
      "outputs": [],
      "source": [
        "# # To make a directory to save all log files in .xlsx for each pre-synaptic (input) and post-synaptic (output) neuron.\n",
        "os.mkdir(\"files_64_raw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4MVxTeX2Njb"
      },
      "outputs": [],
      "source": [
        "def SaveRawFile(dataframe, fol):\n",
        "  \"\"\"\n",
        "  Args :\n",
        "  dataframe: Dataframe\n",
        "  fol : File name of post-synaptic neuron ID\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  columns = dataframe.columns\n",
        "  f = pd.DataFrame()\n",
        "\n",
        "  idx = 0\n",
        "  for e in columns:\n",
        "    \n",
        "    if e.startswith(\"ISI_\"):\n",
        "\n",
        "      if e.startswith(\"ISI_out_\"):\n",
        "          f[e] =  dataframe[e]\n",
        "          f['Avg_ISI_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: getAvgIsi(x))\n",
        "          f['Cov_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
        "          f['Var_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n",
        "        \n",
        "      else:\n",
        "        f[e] =  dataframe[e]\n",
        "        f['Avg_ISI_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: getAvgIsi(x))\n",
        "        f['Cov_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
        "        f['Var_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n",
        "        idx += 1\n",
        "\n",
        "    else:\n",
        "      f[e] = dataframe[e]\n",
        "\n",
        "  f.to_excel(\"files_64_raw/\" + fol[:-4]  + \".xlsx\", index = False)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiXb1EHY2NmS",
        "outputId": "ad7e8304-3ff9-4a42-80f2-b729fc541f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['log_weight0.txt', 'log_weight10.txt', 'log_weight1.txt', 'log_weight11.txt', 'log_weight12.txt', 'log_weight13.txt', 'log_weight14.txt', 'log_weight16.txt', 'log_weight15.txt', 'log_weight17.txt', 'log_weight18.txt', 'log_weight19.txt', 'log_weight20.txt', 'log_weight2.txt', 'log_weight21.txt', 'log_weight22.txt', 'log_weight24.txt', 'log_weight23.txt', 'log_weight25.txt', 'log_weight26.txt', 'log_weight27.txt', 'log_weight28.txt', 'log_weight29.txt', 'log_weight3.txt', 'log_weight30.txt', 'log_weight32.txt', 'log_weight31.txt', 'log_weight33.txt', 'log_weight35.txt', 'log_weight34.txt', 'log_weight36.txt', 'log_weight37.txt', 'log_weight38.txt', 'log_weight39.txt', 'log_weight4.txt', 'log_weight40.txt', 'log_weight42.txt', 'log_weight41.txt', 'log_weight43.txt', 'log_weight44.txt', 'log_weight45.txt', 'log_weight46.txt', 'log_weight47.txt', 'log_weight48.txt', 'log_weight49.txt', 'log_weight5.txt', 'log_weight50.txt', 'log_weight51.txt', 'log_weight52.txt', 'log_weight53.txt', 'log_weight54.txt', 'log_weight55.txt', 'log_weight56.txt', 'log_weight57.txt', 'log_weight58.txt', 'log_weight59.txt', 'log_weight6.txt', 'log_weight61.txt', 'log_weight60.txt', 'log_weight62.txt', 'log_weight63.txt', 'log_weight7.txt', 'log_weight8.txt', 'log_weight9.txt']\n"
          ]
        }
      ],
      "source": [
        "fol_name = os.listdir(\"Output_64x64_precise_1k_logs/\")\n",
        "assert len(fol_name) == 64\n",
        "print(fol_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RXs9QW4Tof"
      },
      "source": [
        "# Saving log files in the desirable and readable format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XQ1uoEVArjF",
        "outputId": "f0035688-54dd-48a2-bb03-9849087bb524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column name : log_weight0.txt having 1 crosspoint/s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return asarray(a).ndim\n",
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[col] = igetitem(value, i)\n",
            "<ipython-input-13-ea7efa8f46f8>:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Avg_ISI_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: getAvgIsi(x))\n",
            "<ipython-input-13-ea7efa8f46f8>:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Cov_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
            "<ipython-input-13-ea7efa8f46f8>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Var_spk_ip_'+str(idx)] =  dataframe['ISI_' + str(idx)].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n",
            "<ipython-input-13-ea7efa8f46f8>:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f[e] = dataframe[e]\n",
            "<ipython-input-13-ea7efa8f46f8>:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f[e] =  dataframe[e]\n",
            "<ipython-input-13-ea7efa8f46f8>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f[e] =  dataframe[e]\n",
            "<ipython-input-13-ea7efa8f46f8>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Avg_ISI_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: getAvgIsi(x))\n",
            "<ipython-input-13-ea7efa8f46f8>:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Cov_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"cov\"))\n",
            "<ipython-input-13-ea7efa8f46f8>:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  f['Var_spk_out_'+e.split(\"ISI_out_\")[-1]] =  dataframe[e].apply(lambda x: cov_or_variance(x, type_metric = \"var\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column name : log_weight1.txt having 2 crosspoint/s.\n",
            "Column name : log_weight10.txt having 11 crosspoint/s.\n",
            "Column name : log_weight11.txt having 12 crosspoint/s.\n",
            "Column name : log_weight12.txt having 13 crosspoint/s.\n",
            "Column name : log_weight13.txt having 14 crosspoint/s.\n",
            "Column name : log_weight14.txt having 15 crosspoint/s.\n",
            "Column name : log_weight15.txt having 16 crosspoint/s.\n",
            "Column name : log_weight16.txt having 17 crosspoint/s.\n",
            "Column name : log_weight17.txt having 18 crosspoint/s.\n",
            "Column name : log_weight18.txt having 19 crosspoint/s.\n",
            "Column name : log_weight19.txt having 20 crosspoint/s.\n",
            "Column name : log_weight2.txt having 3 crosspoint/s.\n",
            "Column name : log_weight20.txt having 21 crosspoint/s.\n",
            "Column name : log_weight21.txt having 22 crosspoint/s.\n",
            "Column name : log_weight22.txt having 23 crosspoint/s.\n",
            "Column name : log_weight23.txt having 24 crosspoint/s.\n",
            "Column name : log_weight24.txt having 25 crosspoint/s.\n",
            "Column name : log_weight25.txt having 26 crosspoint/s.\n",
            "Column name : log_weight26.txt having 27 crosspoint/s.\n",
            "Column name : log_weight27.txt having 28 crosspoint/s.\n",
            "Column name : log_weight28.txt having 29 crosspoint/s.\n",
            "Column name : log_weight29.txt having 30 crosspoint/s.\n",
            "Column name : log_weight3.txt having 4 crosspoint/s.\n",
            "Column name : log_weight30.txt having 31 crosspoint/s.\n",
            "Column name : log_weight31.txt having 32 crosspoint/s.\n",
            "Column name : log_weight32.txt having 33 crosspoint/s.\n",
            "Column name : log_weight33.txt having 34 crosspoint/s.\n",
            "Column name : log_weight34.txt having 35 crosspoint/s.\n",
            "Column name : log_weight35.txt having 36 crosspoint/s.\n",
            "Column name : log_weight36.txt having 37 crosspoint/s.\n",
            "Column name : log_weight37.txt having 38 crosspoint/s.\n",
            "Column name : log_weight38.txt having 39 crosspoint/s.\n",
            "Column name : log_weight39.txt having 40 crosspoint/s.\n",
            "Column name : log_weight4.txt having 5 crosspoint/s.\n",
            "Column name : log_weight40.txt having 41 crosspoint/s.\n",
            "Column name : log_weight41.txt having 42 crosspoint/s.\n",
            "Column name : log_weight42.txt having 43 crosspoint/s.\n",
            "Column name : log_weight43.txt having 44 crosspoint/s.\n",
            "Column name : log_weight44.txt having 45 crosspoint/s.\n",
            "Column name : log_weight45.txt having 46 crosspoint/s.\n",
            "Column name : log_weight46.txt having 47 crosspoint/s.\n",
            "Column name : log_weight47.txt having 48 crosspoint/s.\n",
            "Column name : log_weight48.txt having 49 crosspoint/s.\n",
            "Column name : log_weight49.txt having 50 crosspoint/s.\n",
            "Column name : log_weight5.txt having 6 crosspoint/s.\n",
            "Column name : log_weight50.txt having 51 crosspoint/s.\n",
            "Column name : log_weight51.txt having 52 crosspoint/s.\n",
            "Column name : log_weight52.txt having 53 crosspoint/s.\n",
            "Column name : log_weight53.txt having 54 crosspoint/s.\n",
            "Column name : log_weight54.txt having 55 crosspoint/s.\n",
            "Column name : log_weight55.txt having 56 crosspoint/s.\n",
            "Column name : log_weight56.txt having 57 crosspoint/s.\n",
            "Column name : log_weight57.txt having 58 crosspoint/s.\n",
            "Column name : log_weight58.txt having 59 crosspoint/s.\n",
            "Column name : log_weight59.txt having 60 crosspoint/s.\n",
            "Column name : log_weight6.txt having 7 crosspoint/s.\n",
            "Column name : log_weight60.txt having 61 crosspoint/s.\n",
            "Column name : log_weight61.txt having 62 crosspoint/s.\n",
            "Column name : log_weight62.txt having 63 crosspoint/s.\n",
            "Column name : log_weight63.txt having 64 crosspoint/s.\n",
            "Column name : log_weight7.txt having 8 crosspoint/s.\n",
            "Column name : log_weight8.txt having 9 crosspoint/s.\n",
            "Column name : log_weight9.txt having 10 crosspoint/s.\n"
          ]
        }
      ],
      "source": [
        "for fol in fol_name:\n",
        "  \n",
        "  # Saving raw data in excel file\n",
        "  cp = fol.split(\"log_weight\")[-1][:-4] # Crosspoints\n",
        "  print(f\"Column name : {fol} having {int(cp)+1} crosspoint/s.\")\n",
        "  df = getInfo(\"Output_64x64_precise_1k_logs\"+ '/' + fol, 64)\n",
        "  SaveRawFile(df, fol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwHPqUbL2NrW"
      },
      "outputs": [],
      "source": [
        "# Sort the name of file of post-synaptic neuron ID\n",
        "\n",
        "def atof(text):\n",
        "    try:\n",
        "        retval = float(text)\n",
        "    except ValueError:\n",
        "        retval = text\n",
        "    return retval\n",
        "\n",
        "def natural_keys(text):\n",
        "\n",
        "    return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7sdcsgl2Nuf"
      },
      "outputs": [],
      "source": [
        "# # To create a directory to save all features (Spike rate, Avg. ISI/COV/Variance) and their ground truth (Observed ISI) \n",
        "os.mkdir(\"StackedFiles_avg_ISI\")\n",
        "# os.mkdir(\"StackedFiles_var_spk\")\n",
        "# os.mkdir(\"StackedFiles_cov_spk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRBTQEnz2Nwt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extracting features (Spike rate, Avg. ISI/COV/Variance) and their ground truth (Observed ISI) and \n",
        "save in .npy format.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def StackedFile(path, nG, variance_feat = False):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "    path : File name path for post-synaptic neuron ID\n",
        "    nG : Number of post-synaptic neurons\n",
        "    variance_feat : Coefficient of variation/Variance/Avg ISI as features to be included or not\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    files = [i for i in os.listdir(path) if i.endswith(\".xlsx\")]\n",
        "    files.sort(key=natural_keys)  \n",
        "\n",
        "    \"\"\"\n",
        "    Predicting only average ISI_out, not #Spikes_out\n",
        "    \"\"\"\n",
        "    \n",
        "    for i,f in enumerate(files):\n",
        "\n",
        "      # Number of crosspoints (numCp)\n",
        "      numCp = int(f.split(\"log_weight\")[-1][:-5]) + 1\n",
        "      print(f\"File Name : {f} | Number of crosspoints : {numCp}\")\n",
        "\n",
        "      file = pd.read_excel(path + f)\n",
        "\n",
        "      cols_drop = []\n",
        "\n",
        "      for id in range(nG):\n",
        "        cols_drop.append(\"spk_ip_\" + str(id))\n",
        "        cols_drop.append(\"spk_out_\" + str(id))\n",
        "        cols_drop.append(\"ISI_\" + str(id))\n",
        "        cols_drop.append(\"ISI_out_\" + str(id))\n",
        "        cols_drop.append(\"#Spikes_out_\" + str(id))\n",
        "\n",
        "        if variance_feat == \"var\":\n",
        "          cols_drop.append(\"Cov_spk_ip_\" + str(id))\n",
        "          cols_drop.append(\"Cov_spk_out_\" + str(id))\n",
        "          cols_drop.append(\"Avg_ISI_\" + str(id))\n",
        "          cols_drop.append(\"Avg_ISI_out_\" + str(id))\n",
        "\n",
        "        elif not variance_feat: \n",
        "          cols_drop.append(\"Cov_spk_ip_\" + str(id))\n",
        "          cols_drop.append(\"Cov_spk_out_\" + str(id))\n",
        "          cols_drop.append(\"Var_spk_ip_\" + str(id))\n",
        "          cols_drop.append(\"Var_spk_out_\" + str(id))\n",
        "        else:\n",
        "          cols_drop.append(\"Avg_ISI_\" + str(id))\n",
        "          cols_drop.append(\"Avg_ISI_out_\" + str(id))\n",
        "          cols_drop.append(\"Var_spk_ip_\" + str(id))\n",
        "          cols_drop.append(\"Var_spk_out_\" + str(id))\n",
        "          \n",
        "\n",
        "      cols_drop.extend(['Time(s)'])\n",
        "\n",
        "      file.drop(cols_drop, axis=1, inplace=True)\n",
        "\n",
        "      file_v1 = file.reset_index(drop=True)\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "          savePath = \"StackedFiles_var_spk\"\n",
        "          y_cols = [\"Var_spk_out_\" + str(idx) for idx in range(nG)]\n",
        "      elif not variance_feat: \n",
        "          savePath = \"StackedFiles_avg_ISI\"\n",
        "          y_cols = [\"Avg_ISI_out_\" + str(idx) for idx in range(nG)]\n",
        "      else:\n",
        "          savePath = \"StackedFiles_cov_spk\"\n",
        "          y_cols = [\"Cov_spk_out_\" + str(idx) for idx in range(nG)]\n",
        "          \n",
        "\n",
        "      X_train  = file_v1.drop(y_cols, axis=1).values\n",
        "      y_train =  file_v1[y_cols].values\n",
        "      \n",
        "      X_train_v1 = np.concatenate([X_train[:, :numCp*2], np.zeros((X_train.shape[0], X_train.shape[1] - (numCp*2)))], axis =1)\n",
        "      \n",
        "      if i == 0:\n",
        "        X_tr, y_tr =  X_train_v1, y_train\n",
        "\n",
        "      else:\n",
        "        X_tr1, y_tr1 =  X_train_v1, y_train\n",
        "\n",
        "        X_tr = np.concatenate([X_tr, X_tr1])\n",
        "        y_tr = np.concatenate([y_tr, y_tr1])\n",
        "   \n",
        "      \n",
        "    with open(savePath + \"/X_train_stacked.npy\", 'wb') as f:\n",
        "      np.save(f, X_tr)\n",
        "\n",
        "    for col in range(nG):\n",
        "      with open(savePath +\"/y_train_stacked_\" + str(col) + \".npy\", 'wb') as f:\n",
        "        np.save(f, y_tr[:,col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgfucFxO2Nz4",
        "outputId": "19a04595-723e-4aa6-e067-21002cef08ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Name : log_weight0.xlsx | Number of crosspoints : 1\n",
            "File Name : log_weight1.xlsx | Number of crosspoints : 2\n",
            "File Name : log_weight2.xlsx | Number of crosspoints : 3\n",
            "File Name : log_weight3.xlsx | Number of crosspoints : 4\n",
            "File Name : log_weight4.xlsx | Number of crosspoints : 5\n",
            "File Name : log_weight5.xlsx | Number of crosspoints : 6\n",
            "File Name : log_weight6.xlsx | Number of crosspoints : 7\n",
            "File Name : log_weight7.xlsx | Number of crosspoints : 8\n",
            "File Name : log_weight8.xlsx | Number of crosspoints : 9\n",
            "File Name : log_weight9.xlsx | Number of crosspoints : 10\n",
            "File Name : log_weight10.xlsx | Number of crosspoints : 11\n",
            "File Name : log_weight11.xlsx | Number of crosspoints : 12\n",
            "File Name : log_weight12.xlsx | Number of crosspoints : 13\n",
            "File Name : log_weight13.xlsx | Number of crosspoints : 14\n",
            "File Name : log_weight14.xlsx | Number of crosspoints : 15\n",
            "File Name : log_weight15.xlsx | Number of crosspoints : 16\n",
            "File Name : log_weight16.xlsx | Number of crosspoints : 17\n",
            "File Name : log_weight17.xlsx | Number of crosspoints : 18\n",
            "File Name : log_weight18.xlsx | Number of crosspoints : 19\n",
            "File Name : log_weight19.xlsx | Number of crosspoints : 20\n",
            "File Name : log_weight20.xlsx | Number of crosspoints : 21\n",
            "File Name : log_weight21.xlsx | Number of crosspoints : 22\n",
            "File Name : log_weight22.xlsx | Number of crosspoints : 23\n",
            "File Name : log_weight23.xlsx | Number of crosspoints : 24\n",
            "File Name : log_weight24.xlsx | Number of crosspoints : 25\n",
            "File Name : log_weight25.xlsx | Number of crosspoints : 26\n",
            "File Name : log_weight26.xlsx | Number of crosspoints : 27\n",
            "File Name : log_weight27.xlsx | Number of crosspoints : 28\n",
            "File Name : log_weight28.xlsx | Number of crosspoints : 29\n",
            "File Name : log_weight29.xlsx | Number of crosspoints : 30\n",
            "File Name : log_weight30.xlsx | Number of crosspoints : 31\n",
            "File Name : log_weight31.xlsx | Number of crosspoints : 32\n",
            "File Name : log_weight32.xlsx | Number of crosspoints : 33\n",
            "File Name : log_weight33.xlsx | Number of crosspoints : 34\n",
            "File Name : log_weight34.xlsx | Number of crosspoints : 35\n",
            "File Name : log_weight35.xlsx | Number of crosspoints : 36\n",
            "File Name : log_weight36.xlsx | Number of crosspoints : 37\n",
            "File Name : log_weight37.xlsx | Number of crosspoints : 38\n",
            "File Name : log_weight38.xlsx | Number of crosspoints : 39\n",
            "File Name : log_weight39.xlsx | Number of crosspoints : 40\n",
            "File Name : log_weight40.xlsx | Number of crosspoints : 41\n",
            "File Name : log_weight41.xlsx | Number of crosspoints : 42\n",
            "File Name : log_weight42.xlsx | Number of crosspoints : 43\n",
            "File Name : log_weight43.xlsx | Number of crosspoints : 44\n",
            "File Name : log_weight44.xlsx | Number of crosspoints : 45\n",
            "File Name : log_weight45.xlsx | Number of crosspoints : 46\n",
            "File Name : log_weight46.xlsx | Number of crosspoints : 47\n",
            "File Name : log_weight47.xlsx | Number of crosspoints : 48\n",
            "File Name : log_weight48.xlsx | Number of crosspoints : 49\n",
            "File Name : log_weight49.xlsx | Number of crosspoints : 50\n",
            "File Name : log_weight50.xlsx | Number of crosspoints : 51\n",
            "File Name : log_weight51.xlsx | Number of crosspoints : 52\n",
            "File Name : log_weight52.xlsx | Number of crosspoints : 53\n",
            "File Name : log_weight53.xlsx | Number of crosspoints : 54\n",
            "File Name : log_weight54.xlsx | Number of crosspoints : 55\n",
            "File Name : log_weight55.xlsx | Number of crosspoints : 56\n",
            "File Name : log_weight56.xlsx | Number of crosspoints : 57\n",
            "File Name : log_weight57.xlsx | Number of crosspoints : 58\n",
            "File Name : log_weight58.xlsx | Number of crosspoints : 59\n",
            "File Name : log_weight59.xlsx | Number of crosspoints : 60\n",
            "File Name : log_weight60.xlsx | Number of crosspoints : 61\n",
            "File Name : log_weight61.xlsx | Number of crosspoints : 62\n",
            "File Name : log_weight62.xlsx | Number of crosspoints : 63\n",
            "File Name : log_weight63.xlsx | Number of crosspoints : 64\n"
          ]
        }
      ],
      "source": [
        "# Only considering #Spikes & Avg ISI as features\n",
        "StackedFile(os.getcwd() + '/' + \"files_64_raw/\", 64, variance_feat = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMk5MGyIz35x",
        "outputId": "fa55a351-a6a7-4a1f-b66c-881d00f5d9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Name : log_weight0.xlsx | Number of crosspoints : 1\n",
            "File Name : log_weight1.xlsx | Number of crosspoints : 2\n",
            "File Name : log_weight2.xlsx | Number of crosspoints : 3\n",
            "File Name : log_weight3.xlsx | Number of crosspoints : 4\n",
            "File Name : log_weight4.xlsx | Number of crosspoints : 5\n",
            "File Name : log_weight5.xlsx | Number of crosspoints : 6\n",
            "File Name : log_weight6.xlsx | Number of crosspoints : 7\n",
            "File Name : log_weight7.xlsx | Number of crosspoints : 8\n",
            "File Name : log_weight8.xlsx | Number of crosspoints : 9\n",
            "File Name : log_weight9.xlsx | Number of crosspoints : 10\n",
            "File Name : log_weight10.xlsx | Number of crosspoints : 11\n",
            "File Name : log_weight11.xlsx | Number of crosspoints : 12\n",
            "File Name : log_weight12.xlsx | Number of crosspoints : 13\n",
            "File Name : log_weight13.xlsx | Number of crosspoints : 14\n",
            "File Name : log_weight14.xlsx | Number of crosspoints : 15\n",
            "File Name : log_weight15.xlsx | Number of crosspoints : 16\n",
            "File Name : log_weight16.xlsx | Number of crosspoints : 17\n",
            "File Name : log_weight17.xlsx | Number of crosspoints : 18\n",
            "File Name : log_weight18.xlsx | Number of crosspoints : 19\n",
            "File Name : log_weight19.xlsx | Number of crosspoints : 20\n",
            "File Name : log_weight20.xlsx | Number of crosspoints : 21\n",
            "File Name : log_weight21.xlsx | Number of crosspoints : 22\n",
            "File Name : log_weight22.xlsx | Number of crosspoints : 23\n",
            "File Name : log_weight23.xlsx | Number of crosspoints : 24\n",
            "File Name : log_weight24.xlsx | Number of crosspoints : 25\n",
            "File Name : log_weight25.xlsx | Number of crosspoints : 26\n",
            "File Name : log_weight26.xlsx | Number of crosspoints : 27\n",
            "File Name : log_weight27.xlsx | Number of crosspoints : 28\n",
            "File Name : log_weight28.xlsx | Number of crosspoints : 29\n",
            "File Name : log_weight29.xlsx | Number of crosspoints : 30\n",
            "File Name : log_weight30.xlsx | Number of crosspoints : 31\n",
            "File Name : log_weight31.xlsx | Number of crosspoints : 32\n",
            "File Name : log_weight32.xlsx | Number of crosspoints : 33\n",
            "File Name : log_weight33.xlsx | Number of crosspoints : 34\n",
            "File Name : log_weight34.xlsx | Number of crosspoints : 35\n",
            "File Name : log_weight35.xlsx | Number of crosspoints : 36\n",
            "File Name : log_weight36.xlsx | Number of crosspoints : 37\n",
            "File Name : log_weight37.xlsx | Number of crosspoints : 38\n",
            "File Name : log_weight38.xlsx | Number of crosspoints : 39\n",
            "File Name : log_weight39.xlsx | Number of crosspoints : 40\n",
            "File Name : log_weight40.xlsx | Number of crosspoints : 41\n",
            "File Name : log_weight41.xlsx | Number of crosspoints : 42\n",
            "File Name : log_weight42.xlsx | Number of crosspoints : 43\n",
            "File Name : log_weight43.xlsx | Number of crosspoints : 44\n",
            "File Name : log_weight44.xlsx | Number of crosspoints : 45\n",
            "File Name : log_weight45.xlsx | Number of crosspoints : 46\n",
            "File Name : log_weight46.xlsx | Number of crosspoints : 47\n",
            "File Name : log_weight47.xlsx | Number of crosspoints : 48\n",
            "File Name : log_weight48.xlsx | Number of crosspoints : 49\n",
            "File Name : log_weight49.xlsx | Number of crosspoints : 50\n",
            "File Name : log_weight50.xlsx | Number of crosspoints : 51\n",
            "File Name : log_weight51.xlsx | Number of crosspoints : 52\n",
            "File Name : log_weight52.xlsx | Number of crosspoints : 53\n",
            "File Name : log_weight53.xlsx | Number of crosspoints : 54\n",
            "File Name : log_weight54.xlsx | Number of crosspoints : 55\n",
            "File Name : log_weight55.xlsx | Number of crosspoints : 56\n",
            "File Name : log_weight56.xlsx | Number of crosspoints : 57\n",
            "File Name : log_weight57.xlsx | Number of crosspoints : 58\n",
            "File Name : log_weight58.xlsx | Number of crosspoints : 59\n",
            "File Name : log_weight59.xlsx | Number of crosspoints : 60\n",
            "File Name : log_weight60.xlsx | Number of crosspoints : 61\n",
            "File Name : log_weight61.xlsx | Number of crosspoints : 62\n",
            "File Name : log_weight62.xlsx | Number of crosspoints : 63\n",
            "File Name : log_weight63.xlsx | Number of crosspoints : 64\n"
          ]
        }
      ],
      "source": [
        "# # Only considering #Spikes & Variance of spikes as features\n",
        "# StackedFile(os.getcwd() + '/' + \"files_64_raw/\", 64, variance_feat = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny9k_qqeVUb9",
        "outputId": "2fa7ce67-2f15-4e58-fa75-e68c8b38e4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Name : log_weight0.xlsx | Number of crosspoints : 1\n",
            "File Name : log_weight1.xlsx | Number of crosspoints : 2\n",
            "File Name : log_weight2.xlsx | Number of crosspoints : 3\n",
            "File Name : log_weight3.xlsx | Number of crosspoints : 4\n",
            "File Name : log_weight4.xlsx | Number of crosspoints : 5\n",
            "File Name : log_weight5.xlsx | Number of crosspoints : 6\n",
            "File Name : log_weight6.xlsx | Number of crosspoints : 7\n",
            "File Name : log_weight7.xlsx | Number of crosspoints : 8\n",
            "File Name : log_weight8.xlsx | Number of crosspoints : 9\n",
            "File Name : log_weight9.xlsx | Number of crosspoints : 10\n",
            "File Name : log_weight10.xlsx | Number of crosspoints : 11\n",
            "File Name : log_weight11.xlsx | Number of crosspoints : 12\n",
            "File Name : log_weight12.xlsx | Number of crosspoints : 13\n",
            "File Name : log_weight13.xlsx | Number of crosspoints : 14\n",
            "File Name : log_weight14.xlsx | Number of crosspoints : 15\n",
            "File Name : log_weight15.xlsx | Number of crosspoints : 16\n",
            "File Name : log_weight16.xlsx | Number of crosspoints : 17\n",
            "File Name : log_weight17.xlsx | Number of crosspoints : 18\n",
            "File Name : log_weight18.xlsx | Number of crosspoints : 19\n",
            "File Name : log_weight19.xlsx | Number of crosspoints : 20\n",
            "File Name : log_weight20.xlsx | Number of crosspoints : 21\n",
            "File Name : log_weight21.xlsx | Number of crosspoints : 22\n",
            "File Name : log_weight22.xlsx | Number of crosspoints : 23\n",
            "File Name : log_weight23.xlsx | Number of crosspoints : 24\n",
            "File Name : log_weight24.xlsx | Number of crosspoints : 25\n",
            "File Name : log_weight25.xlsx | Number of crosspoints : 26\n",
            "File Name : log_weight26.xlsx | Number of crosspoints : 27\n",
            "File Name : log_weight27.xlsx | Number of crosspoints : 28\n",
            "File Name : log_weight28.xlsx | Number of crosspoints : 29\n",
            "File Name : log_weight29.xlsx | Number of crosspoints : 30\n",
            "File Name : log_weight30.xlsx | Number of crosspoints : 31\n",
            "File Name : log_weight31.xlsx | Number of crosspoints : 32\n",
            "File Name : log_weight32.xlsx | Number of crosspoints : 33\n",
            "File Name : log_weight33.xlsx | Number of crosspoints : 34\n",
            "File Name : log_weight34.xlsx | Number of crosspoints : 35\n",
            "File Name : log_weight35.xlsx | Number of crosspoints : 36\n",
            "File Name : log_weight36.xlsx | Number of crosspoints : 37\n",
            "File Name : log_weight37.xlsx | Number of crosspoints : 38\n",
            "File Name : log_weight38.xlsx | Number of crosspoints : 39\n",
            "File Name : log_weight39.xlsx | Number of crosspoints : 40\n",
            "File Name : log_weight40.xlsx | Number of crosspoints : 41\n",
            "File Name : log_weight41.xlsx | Number of crosspoints : 42\n",
            "File Name : log_weight42.xlsx | Number of crosspoints : 43\n",
            "File Name : log_weight43.xlsx | Number of crosspoints : 44\n",
            "File Name : log_weight44.xlsx | Number of crosspoints : 45\n",
            "File Name : log_weight45.xlsx | Number of crosspoints : 46\n",
            "File Name : log_weight46.xlsx | Number of crosspoints : 47\n",
            "File Name : log_weight47.xlsx | Number of crosspoints : 48\n",
            "File Name : log_weight48.xlsx | Number of crosspoints : 49\n",
            "File Name : log_weight49.xlsx | Number of crosspoints : 50\n",
            "File Name : log_weight50.xlsx | Number of crosspoints : 51\n",
            "File Name : log_weight51.xlsx | Number of crosspoints : 52\n",
            "File Name : log_weight52.xlsx | Number of crosspoints : 53\n",
            "File Name : log_weight53.xlsx | Number of crosspoints : 54\n",
            "File Name : log_weight54.xlsx | Number of crosspoints : 55\n",
            "File Name : log_weight55.xlsx | Number of crosspoints : 56\n",
            "File Name : log_weight56.xlsx | Number of crosspoints : 57\n",
            "File Name : log_weight57.xlsx | Number of crosspoints : 58\n",
            "File Name : log_weight58.xlsx | Number of crosspoints : 59\n",
            "File Name : log_weight59.xlsx | Number of crosspoints : 60\n",
            "File Name : log_weight60.xlsx | Number of crosspoints : 61\n",
            "File Name : log_weight61.xlsx | Number of crosspoints : 62\n",
            "File Name : log_weight62.xlsx | Number of crosspoints : 63\n",
            "File Name : log_weight63.xlsx | Number of crosspoints : 64\n"
          ]
        }
      ],
      "source": [
        "# # Only considering #Spikes & Variance of spikes as features\n",
        "# StackedFile(os.getcwd() + '/' + \"files_64_raw/\", 64, variance_feat = \"var\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9LeJwK2N2A"
      },
      "outputs": [],
      "source": [
        "def getBinary(y_true, y_pred, margin):\n",
        "  \n",
        "  \"\"\"\n",
        "  It will binarize the continuous values which are y_true and y_pred with\n",
        "  the help of margin and threshold to plot ROC (Receiver operating characteristic).\n",
        "\n",
        "  Args:\n",
        "  y_true : Observed avg. ISI (continuous values)\n",
        "  y_pred : Predicted avg. ISI (continuous values)\n",
        "  margin : The margin is the obtained optimal MAE or a user can take loose margin as per his choice to incur more false alaram or false fault\n",
        "\n",
        "  return:\n",
        "  y_t : Binarized y_true\n",
        "  y_p : Binarized y_pred\n",
        "\n",
        "  The reason to opt for ROC curve because it offers an elegant way to\n",
        "  plot true fault detection rate versus false fault detection rate.\n",
        " \n",
        "  \"\"\"\n",
        "\n",
        "  thresh = pd.Series(y_true).median()   # We have taken median as a threshold because it is not impacted by the outliers\n",
        "  y_t = []\n",
        "  y_p = []\n",
        "\n",
        "  for t,p in zip(y_true, y_pred):\n",
        "    if np.abs(t - p) <= margin:\n",
        "      if t> thresh:\n",
        "        y_t.append(1)\n",
        "        y_p.append(1)\n",
        "      else:\n",
        "        y_t.append(0)\n",
        "        y_p.append(0)\n",
        "\n",
        "    else:\n",
        "      if (t > thresh) and (p > thresh):  # ex: t=60, p=80 but thresh = 55 --> t,p -> 1, therefore one of them has to be opposite to another because abs(t-p)>15.\n",
        "        y_t.append(1)\n",
        "        y_p.append(0)\n",
        "\n",
        "      elif (t > thresh) and (p <= thresh):  # ex: t=60, p=40 but thresh = 55 --> t->1,  p-> 0, which satisfies the condition, one has to be opposite of another.\n",
        "        y_t.append(1)\n",
        "        y_p.append(0)\n",
        "\n",
        "      elif (t <= thresh) and (p > thresh):  # ex: t=30, p=57 but thresh = 55 --> t->0,  p-> 1,\n",
        "        y_t.append(0)\n",
        "        y_p.append(1)\n",
        "\n",
        "      elif (t <= thresh) and (p <= thresh): # ex: t=25, p=45 but thresh = 55 --> t->0, p-> 1, therefore one of them has to be opposite to another.\n",
        "        y_t.append(0)\n",
        "        y_p.append(1)\n",
        "      \n",
        "\n",
        "  y_t = np.array(y_t)\n",
        "  y_p = np.array(y_p)\n",
        "\n",
        "  return y_t, y_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BpLwfl02N49"
      },
      "outputs": [],
      "source": [
        "def plotROC(y_true, y_pred, reg, margin, color):\n",
        "\n",
        "  \"\"\"\n",
        "  To plot ROC\n",
        "\n",
        "  Args: \n",
        "  y_true : Observed avg. ISI\n",
        "  y_pred : Predicted avg. ISI\n",
        "  reg :  Regressor Name\n",
        "  margin : The margin is the obtained optimal MAE or a user can take loose margin as per his choice to incur more false alaram or false fault\n",
        "  color : Color\n",
        "\n",
        "  return:\n",
        "  y_t : Binarized y_true\n",
        "  y_p : Binarized y_pred\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  y_t, y_p = getBinary(y_true, y_pred, margin)\n",
        "  \n",
        "  fpr, tpr, _ = roc_curve(y_t, y_p )\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  print()\n",
        "  print(\"False positive rate : \", fpr[1])\n",
        "  print(\"True positive rate : \", tpr[1])\n",
        "  print(\"ROC Area under curve : \", roc_auc)\n",
        "\n",
        "  plt.figure(figsize = (6,6))\n",
        "  lw = 2\n",
        "  plt.plot(fpr, tpr, color= color, lw=lw, label=f\"ROC curve (area = %0.2f) for {reg}.\" % roc_auc,linewidth=4)\n",
        "  plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "  plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "  plt.title(\"Receiver operating characteristic example\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "  return y_t, y_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCezGNnv2N7b"
      },
      "outputs": [],
      "source": [
        "# # To create a directory for saving scaler and model for each column\n",
        "os.mkdir('Scaler_avg_ISI')\n",
        "os.mkdir('Models_avg_ISI')\n",
        "# os.mkdir('Scaler_cov_spk')\n",
        "# os.mkdir('Models_cov_spk')\n",
        "# os.mkdir('Scaler_var_spk')\n",
        "# os.mkdir('Models_var_spk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w7daBJZ2N93"
      },
      "outputs": [],
      "source": [
        "def DeepNN(type, ip_shape = None):\n",
        "\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  type : ANN or CNN\n",
        "\n",
        "  return:\n",
        "  ANN or CNN model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  hidden_units = 256\n",
        "  n_filters = 16\n",
        "  learning_rate = 0.001 \n",
        "\n",
        "  # Initialize the weights with He normalization\n",
        "  he_init = tf.keras.initializers.HeNormal()\n",
        "\n",
        "  if type == \"CNN\":\n",
        "    # Below hyperparameters of the model are selected by grid search.\n",
        "    model = Sequential([\n",
        "        Conv2D(filters= n_filters, kernel_size=(3, 3), kernel_initializer=he_init, activation='relu', input_shape=ip_shape),BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(filters= n_filters, kernel_size=(1, 1), kernel_initializer=he_init, activation= 'relu'),BatchNormalization(),\n",
        "        MaxPooling2D((1, 1)),\n",
        "        Flatten(),\n",
        "        Dense(hidden_units, kernel_initializer=he_init, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, kernel_initializer=he_init, activation='linear')\n",
        "        ])\n",
        "    \n",
        "    # Loss function\n",
        "    msle =  MeanSquaredError() \n",
        "    model.compile(loss=msle, optimizer=Adam(learning_rate=learning_rate), metrics=[MeanAbsoluteError()])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  elif type == \"ANN\":\n",
        "    # Below hyperparameters of the model are selected by grid search.\n",
        "    model = Sequential([\n",
        "      Dense(hidden_units, kernel_initializer=he_init, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(hidden_units, kernel_initializer=he_init, activation='relu'),\n",
        "      Dropout(0.4),\n",
        "      Dense(1, kernel_initializer=he_init, activation='linear')])\n",
        "    \n",
        "    # Loss function\n",
        "    msle =  MeanSquaredError() \n",
        "    model.compile(loss=msle, optimizer=Adam(learning_rate=learning_rate), metrics=[MeanAbsoluteError()])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7UMP9rN58ni"
      },
      "outputs": [],
      "source": [
        "def training(nG, ip_shape, scaling_mode, variance_feat =False):\n",
        "\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  nG : NUmber of post-synaptic neurons\n",
        "  scaling_mode : Type of scaling technique\n",
        "  ip_shape : Dimension of an example (Mainly for CNN)\n",
        "  variance_feat : Coefficient of variation/Variance/Avg ISI as features included or not\n",
        "\n",
        "  return :\n",
        "  MAE of each model that is trained for each post-synaptic neuron column\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  MAEs = [[], [], [], [], []]\n",
        "  row, col, chn = ip_shape\n",
        "\n",
        "  if variance_feat == \"var\":\n",
        "    with open(\"StackedFiles_var_spk/X_train_stacked.npy\", 'rb') as f:\n",
        "      X = np.load(f)\n",
        "  elif not variance_feat:\n",
        "    with open(\"StackedFiles_avg_ISI/X_train_stacked.npy\", 'rb') as f:\n",
        "      X = np.load(f)\n",
        "  else:\n",
        "    with open(\"StackedFiles_cov_spk/X_train_stacked.npy\", 'rb') as f:\n",
        "      X = np.load(f)\n",
        "\n",
        "\n",
        "  for id in range(nG):\n",
        "\n",
        "    print(\"#\"*25 + \" \" + \"Column\" + str(id) +\" \" + \"#\"*25)\n",
        "\n",
        "    if variance_feat == \"var\":\n",
        "      with open(\"StackedFiles_var_spk/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "        y = np.load(f)\n",
        "        y = y/10000  # (for better fit)\n",
        "\n",
        "    elif not variance_feat:\n",
        "      with open(\"StackedFiles_avg_ISI/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "        y = np.load(f)\n",
        "        y = y/10  # (for better fit)\n",
        "\n",
        "    else:\n",
        "      with open(\"StackedFiles_cov_spk/y_train_stacked_\" + str(id) + \".npy\", 'rb') as f:\n",
        "        y = np.load(f)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Splitting the dataset into training, validation, and test set in (4:0.5:0.5) ratio\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)  \n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=42)\n",
        "\n",
        "    print(\"X_train : \", X_train.shape)\n",
        "    print(\"y_train : \", y_train.shape)\n",
        "    print(\"X_val : \", X_val.shape)\n",
        "    print(\"y_val : \", y_val.shape)\n",
        "    print(\"X_test : \", X_test.shape)\n",
        "    print(\"y_test : \", y_test.shape)\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Standard Scaling\n",
        "    if scaling_mode == 'std':\n",
        "      print(\"Standard Scaling\",'\\n')\n",
        "      scaler = StandardScaler()\n",
        "      x_train_scaled = scaler.fit_transform(X_train)\n",
        "      x_test_scaled = scaler.transform(X_test)\n",
        "      x_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        if id == 0:\n",
        "          with open(os.getcwd() + \"/Scaler_var_spk/\" + \"std_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "      elif not variance_feat:\n",
        "        # Saving scaler for each column\n",
        "        if id == 0:\n",
        "          with open(os.getcwd() + \"/Scaler_avg_ISI/\" + \"std_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "      else:\n",
        "        if id == 0:\n",
        "          with open(os.getcwd() + \"/Scaler_cov_spk/\" + \"std_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "      \n",
        "\n",
        "    # Min-max Scaling\n",
        "    elif scaling_mode == \"minmax\":\n",
        "      print(\"Minmax Scaling \\n\")\n",
        "      scaler = MinMaxScaler()\n",
        "      x_train_scaled = scaler.fit_transform(X_train)\n",
        "      x_test_scaled = scaler.transform(X_test)\n",
        "      x_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        if id == 0:\n",
        "          with open(os.getcwd() + \"/Scaler_var_spk/\" + \"minmax_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "      elif not variance_feat:\n",
        "        if id == 0:\n",
        "          # Saving scaler for each column\n",
        "          with open(os.getcwd() + \"/Scaler_avg_ISI/\" + \"minmax_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "      else:\n",
        "        if id == 0:\n",
        "          with open(os.getcwd() + \"/Scaler_cov_spk/\" + \"minmax_Column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(scaler, file)\n",
        "\n",
        "    # Below hyperparameters of the models are selected by grid search.\n",
        "    if variance_feat == \"var\":\n",
        "      model_name = [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"] \n",
        "      reg1 = GradientBoostingRegressor(learning_rate=0.15, n_estimators=100, min_samples_leaf=1, max_depth=3, random_state=0)\n",
        "      reg2 = LGBMRegressor(n_estimators = 100, boosting_type = 'dart',max_depth = 5, num_leaves = 25, learning_rate=0.3, random_state =42)\n",
        "      reg3 = CatBoostRegressor(learning_rate = 0.05, max_depth = 5,boosting_type = 'Plain',loss_function = \"RMSE\",verbose = False)\n",
        "\n",
        "    elif not variance_feat:\n",
        "      # Training model\n",
        "      model_name = [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"] \n",
        "      reg1 = GradientBoostingRegressor(learning_rate=0.15, n_estimators=100, min_samples_leaf=1, max_depth=3, random_state=0)\n",
        "      reg2 = LGBMRegressor(n_estimators = 100, boosting_type = 'dart',max_depth = 5, num_leaves = 25, learning_rate=0.25, random_state =42)\n",
        "      reg3 = CatBoostRegressor(learning_rate = 0.05, max_depth = 5,boosting_type = 'Plain',loss_function = \"RMSE\",verbose = False)\n",
        "\n",
        "    else:\n",
        "      model_name = [\"XGBRegressor\", \"GradientBoostingRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "      reg1 = XGBRegressor(n_estimators = 100, max_depth = 5, learning_rate=0.03, booster = 'gbtree', tree_method = 'auto',random_state=42)\n",
        "      reg2 = GradientBoostingRegressor(learning_rate=0.15, n_estimators=100, min_samples_leaf=1, max_depth=3, random_state=0)\n",
        "      reg3 = CatBoostRegressor(learning_rate = 0.05, max_depth = 5,boosting_type = 'Plain',loss_function = \"RMSE\",verbose = False)\n",
        "    \n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', verbose=1, patience=5, mode= 'min')\n",
        "    ann = DeepNN('ANN')\n",
        "    cnn = DeepNN('CNN', (row, col, chn))\n",
        "\n",
        "\n",
        "    for ix,reg in enumerate([reg1, reg2, reg3, ann, cnn]): \n",
        "\n",
        "      if len(model_name) -1 == ix:\n",
        "        padding_tr = np.zeros((x_train_scaled.shape[0], 8))\n",
        "        padding_val = np.zeros((x_val_scaled.shape[0], 8))\n",
        "        padding_te = np.zeros((x_test_scaled.shape[0], 8))\n",
        "        x_tr = np.concatenate([padding_tr, x_train_scaled, padding_tr], axis=1).reshape((x_train_scaled.shape[0],row, col, chn))\n",
        "        x_val = np.concatenate([padding_val, x_val_scaled, padding_val], axis=1).reshape((x_val_scaled.shape[0],row, col, chn))\n",
        "        x_te = np.concatenate([padding_te, x_test_scaled, padding_te], axis=1).reshape((x_test_scaled.shape[0],row, col, chn))\n",
        "        reg.fit(x_tr, y_train, epochs=151, batch_size=128, callbacks=[early_stopping], validation_data=(x_val, y_val), verbose=0)\n",
        "        \n",
        "      elif len(model_name)-2 == ix:\n",
        "        reg.fit(x_train_scaled, y_train, epochs=151, batch_size=128, callbacks=[early_stopping], validation_data=(x_val_scaled, y_val), verbose=0)\n",
        "\n",
        "      else:\n",
        "        reg.fit(x_train_scaled, y_train)\n",
        "\n",
        "      if len(model_name) -1 == ix:\n",
        "        tr_mae = np.round(mean_absolute_error(y_train, reg.predict(x_tr)), 5)\n",
        "        val_mae = np.round(mean_absolute_error(y_val, reg.predict(x_val)), 5)\n",
        "        test_mae = np.round(mean_absolute_error(y_test, reg.predict(x_te)), 5)\n",
        "\n",
        "      else:\n",
        "        tr_mae = np.round(mean_absolute_error(y_train, reg.predict(x_train_scaled)), 5)\n",
        "        val_mae = np.round(mean_absolute_error(y_val, reg.predict(x_val_scaled)), 5)\n",
        "        test_mae = np.round(mean_absolute_error(y_test, reg.predict(x_test_scaled)), 5)\n",
        "\n",
        "      reg_name = model_name[ix]\n",
        "\n",
        "      print(f\"MAE of {reg_name} on training set is {tr_mae}.\")\n",
        "      print(f\"MAE of {reg_name} on validation set is {val_mae}.\")\n",
        "      print(f\"MAE of {reg_name} on test set is {test_mae}.\")\n",
        "      print()\n",
        "      print(\"*\"*71)\n",
        "      print()\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        if reg_name == \"GradientBoostingRegressor\":\n",
        "          MAEs[0].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"LGBMRegressor\":\n",
        "          MAEs[1].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CatBoostRegressor\":\n",
        "          MAEs[2].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"ANN\":\n",
        "          MAEs[3].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CNN\":\n",
        "          MAEs[4].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "      elif not variance_feat:\n",
        "        if reg_name == \"GradientBoostingRegressor\":\n",
        "          MAEs[0].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"LGBMRegressor\":\n",
        "          MAEs[1].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CatBoostRegressor\":\n",
        "          MAEs[2].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"ANN\":\n",
        "          MAEs[3].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CNN\":\n",
        "          MAEs[4].append((tr_mae, val_mae, test_mae))\n",
        "      \n",
        "      else:\n",
        "        if reg_name == \"XGBRegressor\":\n",
        "          MAEs[0].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"GradientBoostingRegressor\":\n",
        "          MAEs[1].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CatBoostRegressor\":\n",
        "          MAEs[2].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"ANN\":\n",
        "          MAEs[3].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "        elif reg_name == \"CNN\":\n",
        "          MAEs[4].append((tr_mae, val_mae, test_mae))\n",
        "\n",
        "      if variance_feat == \"var\":\n",
        "        if reg_name in [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\"]:\n",
        "          # Saving regressor model for each column\n",
        "          with open(os.getcwd() + \"/Models_var_spk/\" + reg_name + \"_column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(reg, file)\n",
        "        else:\n",
        "          reg.save(os.getcwd()+ \"/Models_var_spk/\" + reg_name + \"_column_\" + str(id) +\".h5\")\n",
        "\n",
        "\n",
        "      elif not variance_feat:\n",
        "        if reg_name in [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\"]:\n",
        "          # Saving regressor model for each column\n",
        "          with open(os.getcwd() + \"/Models_avg_ISI/\" + reg_name + \"_column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(reg, file)\n",
        "        else:\n",
        "          reg.save(os.getcwd()+ \"/Models_avg_ISI/\" + reg_name + \"_column_\" + str(id) +\".h5\")\n",
        "\n",
        "      else:\n",
        "        if reg_name in [\"XGBRegressor\", \"GradientBoostingRegressor\", \"CatBoostRegressor\"]:\n",
        "          # Saving regressor model for each column\n",
        "          with open(os.getcwd() + \"/Models_cov_spk/\" + reg_name + \"_column_\" + str(id) + \".pkl\" , 'wb') as file:  \n",
        "            pickle.dump(reg, file)\n",
        "        else:\n",
        "          reg.save(os.getcwd()+ \"/Models_cov_spk/\" + reg_name + \"_column_\" + str(id) +\".h5\")\n",
        "\n",
        "\n",
        "  print()\n",
        "  print(\"Results :\\n\")\n",
        "  for ix1,result in enumerate(MAEs):\n",
        "    print(\"*\"*25 + \" \" + model_name[ix1] + \" \" + \"*\"*25)\n",
        "    for col_res in result:\n",
        "      tr_mae, val_mae, test_mae = col_res\n",
        "      print(model_name[ix1] + \" : | Training MAE : \" + str(tr_mae) + \" | Validation MAE : \" + str(val_mae) + \" | Test MAE : \" + str(test_mae))\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    \n",
        "  return MAEs\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEBAu51u58qF"
      },
      "outputs": [],
      "source": [
        "seed_value= 42\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "MAEs_ISI = training(nG = 64, ip_shape = (12,12,1), scaling_mode = \"minmax\", variance_feat = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YevAlsca2oAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79719bb8-5bb8-48f5-c2c4-1de4a1ecd3e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# # # Saving MAEs_ISI to numpy array\n",
        "# with open('MAEs_ISI.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(MAEs_ISI))\n",
        "\n",
        "# Loading MAEs_ISI to numpy array\n",
        "with open('MAEs_ISI.npy', 'rb') as f:\n",
        "    MAEs_ISI = np.load(f)\n",
        "\n",
        "MAEs_ISI.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3rJyXf2kQUf"
      },
      "outputs": [],
      "source": [
        "# seed_value= 42\n",
        "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "# random.seed(seed_value)\n",
        "# np.random.seed(seed_value)\n",
        "# tf.random.set_seed(seed_value)\n",
        "\n",
        "# MAEs_COV = training(nG = 64, ip_shape = (12,12,1), scaling_mode = \"std\", variance_feat = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp6dlLhafVZT"
      },
      "outputs": [],
      "source": [
        "# # # Saving MAEs_COV to numpy array\n",
        "# with open('MAEs_COV.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(MAEs_COV))\n",
        "\n",
        "# Loading MAEs_COV to numpy array\n",
        "# with open('MAEs_COV.npy', 'rb') as f:\n",
        "#     MAEs_COV = np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeYHTYZxfVmk"
      },
      "outputs": [],
      "source": [
        "# seed_value= 42\n",
        "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "# random.seed(seed_value)\n",
        "# np.random.seed(seed_value)\n",
        "# tf.random.set_seed(seed_value)\n",
        "\n",
        "# MAEs_VAR = training(nG = 64, ip_shape = (12,12,1), scaling_mode = \"std\", variance_feat = \"var\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6941SRFNfWCy"
      },
      "outputs": [],
      "source": [
        "# # Saving MAEs_VAR to numpy array\n",
        "# with open('MAEs_VAR.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(MAEs_VAR))\n",
        "\n",
        "# # Loading MAEs_VAR to numpy array\n",
        "# with open('MAEs_VAR.npy', 'rb') as f:\n",
        "#     MAEs_VAR = np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LPy4V8kS1bI"
      },
      "outputs": [],
      "source": [
        "def getMu_Std(maes):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  maes :  Mean absolute error for all columns for each model\n",
        "\n",
        "  return :\n",
        "  mu, std : Mean and Standard deviation of performance of each model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Fetching MAE of test set across each column for each model\n",
        "  mae_te = [[j[-1] for j in i] for i in maes]\n",
        "\n",
        "  # Mean of MAE of test set\n",
        "  mu = np.mean(mae_te, axis=1)\n",
        "\n",
        "  # Standard deviation of MAE of test set\n",
        "  std = np.sqrt(np.var(mae_te, axis=1))\n",
        "\n",
        "  return mu, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKU6-I0JS1eU"
      },
      "outputs": [],
      "source": [
        "# Considering only #Spikes, Avg ISI as features\n",
        "# [\"GradientBoostingRegressor\", \"LGBMRegressor\", \"CatBoostRegressor\", \"ANN\", \"CNN\"]\n",
        "mu_ISI, std_ISI = getMu_Std(MAEs_ISI)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-voE35C1esM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owqoPMd-1exP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}